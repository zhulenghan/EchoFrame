{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import torch\n",
    "import laion_clap\n",
    "from extraction.vgg_sound import *\n",
    "from audioldm import image_to_audio, build_model, clap_to_audio\n",
    "import torch\n",
    "import torchaudio\n",
    "from transformers import AutoProcessor, AutoModel\n",
    "import soundfile as sf \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/vocab.json HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading HTSAT-tiny model config.\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO:root:Loading pretrained HTSAT-tiny-roberta weights (/home/ubuntu/project/v2a-mapper/pretrain/clap_htsat_tiny.pt).\n"
     ]
    }
   ],
   "source": [
    "from audioldm.clap.encoders import CLAPAudioEmbeddingClassifierFreev2\n",
    "CLAP = CLAPAudioEmbeddingClassifierFreev2(\n",
    "    key='waveform',\n",
    "    pretrained_path=\"/home/ubuntu/project/v2a-mapper/pretrain/clap_htsat_tiny.pt\",\n",
    "    sampling_rate=16000,\n",
    "    embed_mode=\"audio\",\n",
    "    amodel=\"HTSAT-tiny\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:17 videos found in /home/ubuntu/project/subdata/video\n",
      "INFO:root:17 videos found in /home/ubuntu/project/subdata/train_subset.csv\n",
      "INFO:root:48 videos missing in /home/ubuntu/project/subdata/video\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_rate: 44100\n",
      "self sample:  16000\n"
     ]
    }
   ],
   "source": [
    "vgg_dataset = VGGSound(root=\"/home/ubuntu/project/subdata/video\",\n",
    "                           csv_path=\"/home/ubuntu/project/subdata/train_subset.csv\",\n",
    "                           sample_rate =  16000)\n",
    "\n",
    "data = vgg_dataset[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '1JWJSFMGrN4_000030.mp4',\n",
       " 'caption': 'male singing',\n",
       " 'audio': tensor([-0.0032,  0.0149,  0.0367,  ...,  0.0535,  0.0584,  0.0576]),\n",
       " 'clip_video': Image([[[[0.0627, 0.0627, 0.0627,  ..., 0.0627, 0.0627, 0.0627],\n",
       "          [0.0627, 0.0627, 0.0627,  ..., 0.0627, 0.0627, 0.0627],\n",
       "          [0.0627, 0.0627, 0.0627,  ..., 0.0627, 0.0627, 0.0627],\n",
       "          ...,\n",
       "          [0.0627, 0.0627, 0.0627,  ..., 0.0627, 0.0627, 0.0627],\n",
       "          [0.0627, 0.0627, 0.0627,  ..., 0.0627, 0.0627, 0.0627],\n",
       "          [0.0627, 0.0627, 0.0627,  ..., 0.0627, 0.0627, 0.0627]],\n",
       " \n",
       "         [[0.0627, 0.0627, 0.0627,  ..., 0.0627, 0.0627, 0.0627],\n",
       "          [0.0627, 0.0627, 0.0627,  ..., 0.0627, 0.0627, 0.0627],\n",
       "          [0.0627, 0.0627, 0.0627,  ..., 0.0627, 0.0627, 0.0627],\n",
       "          ...,\n",
       "          [0.0627, 0.0627, 0.0627,  ..., 0.0627, 0.0627, 0.0627],\n",
       "          [0.0627, 0.0627, 0.0627,  ..., 0.0627, 0.0627, 0.0627],\n",
       "          [0.0627, 0.0627, 0.0627,  ..., 0.0627, 0.0627, 0.0627]],\n",
       " \n",
       "         [[0.0627, 0.0627, 0.0627,  ..., 0.0627, 0.0627, 0.0627],\n",
       "          [0.0627, 0.0627, 0.0627,  ..., 0.0627, 0.0627, 0.0627],\n",
       "          [0.0627, 0.0627, 0.0627,  ..., 0.0627, 0.0627, 0.0627],\n",
       "          ...,\n",
       "          [0.0627, 0.0627, 0.0627,  ..., 0.0627, 0.0627, 0.0627],\n",
       "          [0.0627, 0.0627, 0.0627,  ..., 0.0627, 0.0627, 0.0627],\n",
       "          [0.0627, 0.0627, 0.0627,  ..., 0.0627, 0.0627, 0.0627]]],\n",
       " \n",
       " \n",
       "        [[[0.0627, 0.0627, 0.0627,  ..., 0.0627, 0.0627, 0.0627],\n",
       "          [0.0627, 0.0627, 0.0627,  ..., 0.0627, 0.0627, 0.0627],\n",
       "          [0.0627, 0.0627, 0.0627,  ..., 0.0627, 0.0627, 0.0627],\n",
       "          ...,\n",
       "          [0.0627, 0.0627, 0.0627,  ..., 0.0627, 0.0627, 0.0627],\n",
       "          [0.0627, 0.0627, 0.0627,  ..., 0.0627, 0.0627, 0.0627],\n",
       "          [0.0627, 0.0627, 0.0627,  ..., 0.0627, 0.0627, 0.0627]],\n",
       " \n",
       "         [[0.0627, 0.0627, 0.0627,  ..., 0.0627, 0.0627, 0.0627],\n",
       "          [0.0627, 0.0627, 0.0627,  ..., 0.0627, 0.0627, 0.0627],\n",
       "          [0.0627, 0.0627, 0.0627,  ..., 0.0627, 0.0627, 0.0627],\n",
       "          ...,\n",
       "          [0.0627, 0.0627, 0.0627,  ..., 0.0627, 0.0627, 0.0627],\n",
       "          [0.0627, 0.0627, 0.0627,  ..., 0.0627, 0.0627, 0.0627],\n",
       "          [0.0627, 0.0627, 0.0627,  ..., 0.0627, 0.0627, 0.0627]],\n",
       " \n",
       "         [[0.0627, 0.0627, 0.0627,  ..., 0.0627, 0.0627, 0.0627],\n",
       "          [0.0627, 0.0627, 0.0627,  ..., 0.0627, 0.0627, 0.0627],\n",
       "          [0.0627, 0.0627, 0.0627,  ..., 0.0627, 0.0627, 0.0627],\n",
       "          ...,\n",
       "          [0.0627, 0.0627, 0.0627,  ..., 0.0627, 0.0627, 0.0627],\n",
       "          [0.0627, 0.0627, 0.0627,  ..., 0.0627, 0.0627, 0.0627],\n",
       "          [0.0627, 0.0627, 0.0627,  ..., 0.0627, 0.0627, 0.0627]]],\n",
       " \n",
       " \n",
       "        [[[0.0627, 0.0627, 0.0627,  ..., 0.0627, 0.0627, 0.0627],\n",
       "          [0.0627, 0.0627, 0.0627,  ..., 0.0627, 0.0627, 0.0627],\n",
       "          [0.0627, 0.0627, 0.0627,  ..., 0.0627, 0.0627, 0.0627],\n",
       "          ...,\n",
       "          [0.0627, 0.0627, 0.0627,  ..., 0.0627, 0.0627, 0.0627],\n",
       "          [0.0627, 0.0627, 0.0627,  ..., 0.0627, 0.0627, 0.0627],\n",
       "          [0.0627, 0.0627, 0.0627,  ..., 0.0627, 0.0627, 0.0627]],\n",
       " \n",
       "         [[0.0627, 0.0627, 0.0627,  ..., 0.0627, 0.0627, 0.0627],\n",
       "          [0.0627, 0.0627, 0.0627,  ..., 0.0627, 0.0627, 0.0627],\n",
       "          [0.0627, 0.0627, 0.0627,  ..., 0.0627, 0.0627, 0.0627],\n",
       "          ...,\n",
       "          [0.0627, 0.0627, 0.0627,  ..., 0.0627, 0.0627, 0.0627],\n",
       "          [0.0627, 0.0627, 0.0627,  ..., 0.0627, 0.0627, 0.0627],\n",
       "          [0.0627, 0.0627, 0.0627,  ..., 0.0627, 0.0627, 0.0627]],\n",
       " \n",
       "         [[0.0627, 0.0627, 0.0627,  ..., 0.0627, 0.0627, 0.0627],\n",
       "          [0.0627, 0.0627, 0.0627,  ..., 0.0627, 0.0627, 0.0627],\n",
       "          [0.0627, 0.0627, 0.0627,  ..., 0.0627, 0.0627, 0.0627],\n",
       "          ...,\n",
       "          [0.0627, 0.0627, 0.0627,  ..., 0.0627, 0.0627, 0.0627],\n",
       "          [0.0627, 0.0627, 0.0627,  ..., 0.0627, 0.0627, 0.0627],\n",
       "          [0.0627, 0.0627, 0.0627,  ..., 0.0627, 0.0627, 0.0627]]],\n",
       " \n",
       " \n",
       "        ...,\n",
       " \n",
       " \n",
       "        [[[0.4157, 0.4196, 0.4392,  ..., 0.2902, 0.3059, 0.3059],\n",
       "          [0.4118, 0.4235, 0.4392,  ..., 0.3098, 0.3020, 0.3176],\n",
       "          [0.4157, 0.4235, 0.4392,  ..., 0.3137, 0.2980, 0.3020],\n",
       "          ...,\n",
       "          [0.6667, 0.6784, 0.6745,  ..., 0.5647, 0.5882, 0.5922],\n",
       "          [0.6314, 0.6392, 0.6510,  ..., 0.6000, 0.6118, 0.6157],\n",
       "          [0.6392, 0.6471, 0.6627,  ..., 0.6235, 0.6353, 0.6471]],\n",
       " \n",
       "         [[0.7412, 0.7451, 0.7569,  ..., 0.4941, 0.5098, 0.5098],\n",
       "          [0.7373, 0.7490, 0.7529,  ..., 0.5137, 0.5059, 0.5216],\n",
       "          [0.7412, 0.7490, 0.7529,  ..., 0.5176, 0.5020, 0.5059],\n",
       "          ...,\n",
       "          [0.8627, 0.8745, 0.8706,  ..., 0.7882, 0.7922, 0.7961],\n",
       "          [0.8275, 0.8353, 0.8471,  ..., 0.8078, 0.8039, 0.8039],\n",
       "          [0.8353, 0.8431, 0.8588,  ..., 0.8314, 0.8314, 0.8314]],\n",
       " \n",
       "         [[0.8824, 0.8863, 0.9020,  ..., 0.7569, 0.7725, 0.7725],\n",
       "          [0.8784, 0.8902, 0.8980,  ..., 0.7765, 0.7686, 0.7843],\n",
       "          [0.8824, 0.8902, 0.8980,  ..., 0.7804, 0.7647, 0.7686],\n",
       "          ...,\n",
       "          [0.9608, 0.9725, 0.9686,  ..., 0.9451, 0.9569, 0.9608],\n",
       "          [0.9255, 0.9333, 0.9451,  ..., 0.9725, 0.9725, 0.9725],\n",
       "          [0.9333, 0.9412, 0.9569,  ..., 0.9961, 0.9961, 0.9961]]],\n",
       " \n",
       " \n",
       "        [[[0.4157, 0.4196, 0.4392,  ..., 0.2902, 0.3059, 0.3059],\n",
       "          [0.4118, 0.4235, 0.4392,  ..., 0.3098, 0.3020, 0.3176],\n",
       "          [0.4157, 0.4235, 0.4392,  ..., 0.3137, 0.2980, 0.3020],\n",
       "          ...,\n",
       "          [0.6667, 0.6784, 0.6784,  ..., 0.5647, 0.5882, 0.5922],\n",
       "          [0.6314, 0.6392, 0.6510,  ..., 0.6000, 0.6118, 0.6157],\n",
       "          [0.6392, 0.6471, 0.6627,  ..., 0.6235, 0.6353, 0.6471]],\n",
       " \n",
       "         [[0.7412, 0.7451, 0.7569,  ..., 0.4941, 0.5098, 0.5098],\n",
       "          [0.7373, 0.7490, 0.7529,  ..., 0.5137, 0.5059, 0.5216],\n",
       "          [0.7412, 0.7490, 0.7529,  ..., 0.5176, 0.5020, 0.5059],\n",
       "          ...,\n",
       "          [0.8627, 0.8745, 0.8745,  ..., 0.7882, 0.7922, 0.7961],\n",
       "          [0.8275, 0.8353, 0.8471,  ..., 0.8078, 0.8039, 0.8039],\n",
       "          [0.8353, 0.8431, 0.8588,  ..., 0.8314, 0.8314, 0.8314]],\n",
       " \n",
       "         [[0.8824, 0.8863, 0.9020,  ..., 0.7569, 0.7725, 0.7725],\n",
       "          [0.8784, 0.8902, 0.8980,  ..., 0.7765, 0.7686, 0.7843],\n",
       "          [0.8824, 0.8902, 0.8980,  ..., 0.7804, 0.7647, 0.7686],\n",
       "          ...,\n",
       "          [0.9608, 0.9725, 0.9765,  ..., 0.9451, 0.9569, 0.9608],\n",
       "          [0.9255, 0.9333, 0.9451,  ..., 0.9725, 0.9725, 0.9725],\n",
       "          [0.9333, 0.9412, 0.9569,  ..., 0.9961, 0.9961, 0.9961]]],\n",
       " \n",
       " \n",
       "        [[[0.4157, 0.4196, 0.4392,  ..., 0.2902, 0.3059, 0.3059],\n",
       "          [0.4118, 0.4235, 0.4392,  ..., 0.3098, 0.3020, 0.3176],\n",
       "          [0.4157, 0.4235, 0.4392,  ..., 0.3137, 0.2980, 0.3020],\n",
       "          ...,\n",
       "          [0.6667, 0.6784, 0.6784,  ..., 0.5647, 0.5882, 0.5922],\n",
       "          [0.6314, 0.6392, 0.6510,  ..., 0.6000, 0.6118, 0.6157],\n",
       "          [0.6392, 0.6471, 0.6627,  ..., 0.6235, 0.6353, 0.6471]],\n",
       " \n",
       "         [[0.7412, 0.7451, 0.7569,  ..., 0.4941, 0.5098, 0.5098],\n",
       "          [0.7373, 0.7490, 0.7529,  ..., 0.5137, 0.5059, 0.5216],\n",
       "          [0.7412, 0.7490, 0.7529,  ..., 0.5176, 0.5020, 0.5059],\n",
       "          ...,\n",
       "          [0.8627, 0.8745, 0.8745,  ..., 0.7882, 0.7922, 0.7961],\n",
       "          [0.8275, 0.8353, 0.8471,  ..., 0.8078, 0.8039, 0.8039],\n",
       "          [0.8353, 0.8431, 0.8588,  ..., 0.8314, 0.8314, 0.8314]],\n",
       " \n",
       "         [[0.8824, 0.8863, 0.9020,  ..., 0.7569, 0.7725, 0.7725],\n",
       "          [0.8784, 0.8902, 0.8980,  ..., 0.7765, 0.7686, 0.7843],\n",
       "          [0.8824, 0.8902, 0.8980,  ..., 0.7804, 0.7647, 0.7686],\n",
       "          ...,\n",
       "          [0.9608, 0.9725, 0.9765,  ..., 0.9451, 0.9569, 0.9608],\n",
       "          [0.9255, 0.9333, 0.9451,  ..., 0.9725, 0.9725, 0.9725],\n",
       "          [0.9333, 0.9412, 0.9569,  ..., 0.9961, 0.9961, 0.9961]]]], )}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio_bs = data['audio'].unsqueeze(0).unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/v2a-mapper/lib/python3.11/site-packages/torchaudio/transforms/_transforms.py:580: UserWarning: Argument 'onesided' has been deprecated and has no influence on the behavior of this module.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "embed = CLAP.forward(data['audio'].unsqueeze(0).to('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.1245e-03,  1.6069e-02,  5.5601e-02, -6.2583e-02, -5.0328e-02,\n",
       "          4.1050e-02,  2.2514e-02, -2.7365e-02,  1.3244e-02,  2.7520e-02,\n",
       "         -2.5884e-02, -4.3049e-02,  4.5110e-02,  7.9565e-02,  3.4114e-02,\n",
       "         -6.2841e-03,  5.0500e-03,  1.8683e-02,  2.3052e-02, -3.1869e-02,\n",
       "          3.4993e-03,  2.1129e-02, -1.9924e-02,  3.6578e-03,  2.5397e-02,\n",
       "         -5.9745e-02, -7.1013e-02,  1.3497e-02, -2.6585e-03,  6.6438e-02,\n",
       "          2.5386e-02,  6.4305e-03, -3.0029e-02, -3.7125e-02,  5.4214e-02,\n",
       "          3.8048e-02, -6.9573e-02, -6.5653e-02,  1.7121e-02,  3.6397e-02,\n",
       "          3.4468e-02, -4.2671e-02, -4.7466e-03,  2.1016e-02, -6.6543e-02,\n",
       "         -2.0464e-02, -3.7350e-02, -2.8601e-02, -3.7733e-02,  2.7174e-02,\n",
       "          1.1792e-03,  4.0551e-02, -3.2220e-02, -8.6845e-02, -1.1159e-02,\n",
       "         -2.7319e-02,  8.0420e-02, -1.9781e-02, -1.8699e-03, -4.4303e-02,\n",
       "         -1.4986e-02, -9.9876e-03,  5.8313e-02,  7.5958e-02, -1.4166e-02,\n",
       "          2.2849e-02, -4.5340e-02,  3.2655e-02, -5.7856e-03, -2.2854e-02,\n",
       "          3.2924e-02, -4.1128e-02, -9.9197e-03, -6.2035e-02,  6.7907e-02,\n",
       "          1.3126e-02, -7.1476e-02, -1.5431e-02,  1.4407e-02, -1.4348e-02,\n",
       "          1.4977e-02, -1.8650e-02, -1.1083e-02,  1.0119e-02, -9.0725e-02,\n",
       "          6.8961e-02,  7.0937e-03, -2.6398e-02,  1.7523e-02,  7.9483e-02,\n",
       "          2.7800e-02,  2.4447e-02, -3.3177e-02, -2.1096e-02,  3.3716e-02,\n",
       "          8.9158e-02,  2.6347e-02,  4.0986e-02,  4.7211e-02, -4.9134e-03,\n",
       "          1.8567e-02,  3.0245e-02, -5.5873e-02,  2.4435e-02,  6.9209e-02,\n",
       "          2.5821e-02, -5.3750e-03, -3.8593e-02, -8.2710e-03, -9.9414e-03,\n",
       "          2.4613e-02,  3.4086e-02,  5.9379e-03,  1.1905e-03,  4.7455e-02,\n",
       "         -2.5680e-02,  3.4858e-02,  1.8535e-02, -2.3095e-02,  4.8930e-02,\n",
       "         -6.0352e-02,  2.1697e-02, -1.8150e-02,  2.5905e-02,  9.7510e-02,\n",
       "          1.8985e-02,  5.6027e-02, -1.6542e-02, -3.4853e-02, -8.1226e-02,\n",
       "          2.1387e-02, -3.8545e-03, -3.3661e-02,  1.3789e-02,  2.6797e-02,\n",
       "          1.3261e-02,  7.0384e-02, -5.9830e-02, -6.1570e-02,  2.2080e-02,\n",
       "         -6.7773e-02,  8.7767e-02, -2.4273e-02, -5.5713e-02,  4.5751e-02,\n",
       "         -5.7310e-02, -1.4976e-02, -6.2703e-02,  1.2329e-03,  1.0548e-02,\n",
       "         -4.2416e-02,  6.0140e-02, -2.4513e-02, -8.1367e-02,  5.1918e-02,\n",
       "          3.0060e-02, -1.5326e-02, -7.3174e-02, -3.5367e-02, -6.1913e-02,\n",
       "          5.7766e-03, -1.5842e-02, -6.9551e-02, -5.8500e-02, -1.7654e-02,\n",
       "         -3.9700e-02,  3.2079e-02,  8.6348e-02,  3.2268e-02, -2.7717e-02,\n",
       "         -6.8901e-03,  4.1287e-02, -5.5682e-02,  1.7552e-02,  2.9140e-02,\n",
       "          4.1073e-02,  5.5026e-02, -8.1425e-03,  6.9360e-03,  1.1572e-02,\n",
       "          2.5655e-02,  3.7738e-02,  7.0326e-03,  6.0269e-02, -4.2763e-02,\n",
       "          1.4089e-02,  7.5173e-03,  4.3513e-02, -1.2183e-02,  6.5351e-02,\n",
       "         -7.5896e-02, -7.2175e-02,  9.1443e-03,  1.7739e-02,  6.1839e-02,\n",
       "          5.3442e-02,  8.3226e-02, -3.0785e-02, -1.8440e-02,  4.7106e-02,\n",
       "          4.4251e-02, -1.2675e-03, -2.0886e-02, -7.0373e-03, -1.8694e-02,\n",
       "          3.6422e-02, -3.6368e-02,  3.0762e-02, -3.9598e-02, -1.2169e-02,\n",
       "          1.9706e-02, -3.2754e-02,  7.4239e-02,  4.3762e-02,  2.1488e-02,\n",
       "         -4.0607e-02,  6.1594e-03, -9.6732e-03, -9.7554e-03,  3.8623e-02,\n",
       "          5.2364e-02, -1.9348e-02,  1.5790e-02, -2.2250e-02, -7.0934e-02,\n",
       "          8.4007e-02, -4.0021e-02,  3.9689e-02,  2.9442e-02, -6.4930e-04,\n",
       "         -3.4869e-02,  1.4742e-02, -6.9373e-03, -1.4885e-02,  5.0496e-02,\n",
       "         -1.0603e-01, -3.7106e-02,  5.2070e-03,  1.3762e-02,  7.3703e-03,\n",
       "          2.5898e-02,  2.4931e-02, -3.7710e-02, -7.4194e-03, -5.0997e-02,\n",
       "          3.1833e-02,  5.9243e-02,  7.3984e-04, -1.9819e-02, -2.0736e-02,\n",
       "          1.5187e-02, -1.9731e-02, -7.7614e-04, -1.1498e-02, -2.6484e-02,\n",
       "         -2.4780e-02,  5.6880e-02,  6.6176e-02,  4.2637e-03,  3.4356e-03,\n",
       "         -3.9669e-02,  2.5663e-02, -3.3586e-02,  1.1640e-01, -3.0248e-02,\n",
       "         -1.2779e-02,  4.3351e-02, -5.8779e-02, -2.9965e-02, -1.1549e-01,\n",
       "         -2.9411e-02, -9.9557e-03, -2.5335e-02,  3.1312e-02, -2.9837e-02,\n",
       "         -5.5419e-03,  6.7420e-03, -4.1386e-02,  8.1140e-02, -3.8637e-02,\n",
       "          1.5462e-02, -8.2324e-02, -1.1189e-02, -2.7848e-03, -3.4198e-02,\n",
       "          4.4577e-02,  5.3233e-03,  7.1741e-02,  1.9941e-02,  5.5435e-03,\n",
       "         -1.7195e-04,  1.0097e-01,  6.1932e-02,  2.8675e-02,  2.7303e-02,\n",
       "         -6.6224e-02,  3.5611e-02,  2.2971e-02, -2.8462e-02,  6.3242e-02,\n",
       "          4.5992e-02, -3.8379e-02, -4.7602e-02,  5.0465e-02, -8.9315e-02,\n",
       "          5.8766e-03,  6.1480e-02,  6.3512e-02, -4.1970e-02,  2.4944e-02,\n",
       "          9.2729e-02,  1.4988e-01, -2.7973e-02,  1.4085e-03, -1.0146e-01,\n",
       "          7.1051e-02, -7.0738e-03, -4.2615e-02, -6.7891e-02,  1.3653e-02,\n",
       "          7.5885e-02,  9.8427e-03, -6.6290e-03,  1.7880e-02,  8.8528e-03,\n",
       "         -7.2971e-02, -1.7258e-02, -2.2963e-02, -3.3012e-03,  1.1165e-02,\n",
       "          5.8850e-05,  6.9002e-03,  6.1351e-02,  5.5851e-02, -1.3187e-02,\n",
       "          7.5097e-03, -6.5051e-02,  3.2927e-02, -2.4642e-02, -5.9581e-03,\n",
       "          6.0466e-02,  5.6474e-02,  6.6719e-03, -1.3253e-02,  6.6748e-02,\n",
       "         -2.2457e-02, -3.5472e-02, -7.2690e-02,  6.9658e-02, -8.4992e-02,\n",
       "         -1.2132e-02,  3.9129e-02,  3.2458e-02,  4.0249e-02,  6.8585e-02,\n",
       "          1.5166e-02,  5.9448e-02,  2.6765e-03,  1.0149e-02,  7.3780e-02,\n",
       "          1.3048e-02,  1.4849e-02,  8.2102e-02,  7.0249e-02, -5.0442e-02,\n",
       "          5.1238e-02,  6.3553e-03, -3.9378e-02,  2.0093e-02,  3.5316e-02,\n",
       "          4.5924e-02,  4.2255e-02, -1.0693e-02, -2.3790e-02, -2.5319e-02,\n",
       "         -2.2578e-02,  4.2062e-02,  3.0249e-02,  3.1576e-02,  4.9283e-02,\n",
       "          4.5913e-02, -2.4761e-02,  1.9558e-03, -3.2586e-02, -2.2133e-02,\n",
       "         -1.0871e-02,  5.2160e-02, -1.8923e-02,  9.9754e-03, -4.7242e-03,\n",
       "         -4.0695e-02,  3.6345e-02,  1.0307e-01, -2.1219e-03, -7.7716e-02,\n",
       "          1.4909e-02,  2.0771e-02,  3.7596e-02,  2.3009e-02, -1.0314e-02,\n",
       "          3.3348e-03, -1.8397e-02, -2.7588e-02,  1.8595e-02, -1.1386e-01,\n",
       "          4.8417e-02, -4.9010e-02,  6.4228e-02, -2.9263e-02, -3.0714e-02,\n",
       "          1.0088e-01, -1.2216e-03,  7.1489e-02,  2.2239e-03, -7.4926e-02,\n",
       "         -3.6042e-03,  3.3268e-02,  5.2928e-02,  2.4342e-02, -9.9666e-04,\n",
       "         -1.7700e-02, -6.1250e-03, -5.8666e-03, -4.2795e-02, -3.3530e-02,\n",
       "          2.4968e-02, -1.9410e-02,  4.3944e-02, -2.6671e-02, -2.4785e-02,\n",
       "         -4.1728e-02,  9.0492e-03, -3.1673e-03,  4.8355e-02, -1.1422e-02,\n",
       "          2.9468e-02,  2.3375e-03, -6.3088e-02,  4.3475e-02,  5.1437e-02,\n",
       "         -3.9424e-02,  1.1339e-02,  1.9229e-02,  8.3237e-02, -4.6138e-02,\n",
       "          1.2530e-03,  3.1809e-02,  1.1059e-02, -1.1875e-02, -1.0353e-03,\n",
       "          4.1538e-02, -2.9345e-02, -3.8589e-02,  3.8909e-03, -1.2625e-02,\n",
       "          4.2393e-02,  1.2112e-02,  3.3888e-02,  8.5425e-02, -1.3430e-01,\n",
       "         -1.9667e-02, -3.6625e-02, -9.5890e-02,  3.5582e-02, -1.1391e-02,\n",
       "          6.0953e-02, -6.8202e-02, -3.6740e-02,  7.7492e-02,  7.7006e-03,\n",
       "          1.0407e-01, -6.8813e-02, -7.5982e-02,  1.0220e-01, -4.0865e-02,\n",
       "         -8.3825e-02,  5.7991e-03, -1.1729e-02, -6.7069e-03, -4.5889e-02,\n",
       "          2.5790e-03,  3.3951e-02,  5.4669e-02, -1.2070e-02,  2.2999e-02,\n",
       "          4.2189e-04,  3.6969e-02, -3.5051e-02, -2.0651e-03,  1.4747e-02,\n",
       "          3.1025e-02, -1.7105e-02,  1.0759e-01,  5.6544e-03, -4.9747e-02,\n",
       "         -2.0254e-02, -9.3859e-03,  4.0623e-02,  2.1217e-02, -6.2213e-02,\n",
       "         -1.1873e-02,  4.4248e-02,  8.2298e-03,  8.0253e-03,  1.0903e-01,\n",
       "          7.9363e-02, -4.1871e-02,  1.4006e-02,  3.7225e-03, -3.2389e-02,\n",
       "         -2.4511e-02, -1.3219e-01]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = data['clip_video'][0, :, :, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processor = AutoProcessor.from_pretrained(\"laion/clap-htsat-unfused\")\n",
    "# model = AutoModel.from_pretrained(\"laion/clap-htsat-unfused\")\n",
    "\n",
    "\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# model = model.to(device)\n",
    "# model.eval()\n",
    "\n",
    "# # # Load audio file\n",
    "# # audio_path = \"input_audio.wav\"\n",
    "# # waveform, sr = torchaudio.load(audio_path)\n",
    "# waveform = data['audio']\n",
    "\n",
    "# # Resample to 48kHz (required by CLAP)\n",
    "# # if sr != 48000:\n",
    "# # resampler = torchaudio.transforms.Resample(16000, 48000)\n",
    "# # waveform = resampler(waveform)\n",
    "\n",
    "# # # Convert to mono\n",
    "# # if waveform.shape[0] > 1:\n",
    "# #     waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "\n",
    "# # Prepare inputs for the model\n",
    "# inputs = processor(audios=waveform,sampling_rate = 48000, return_tensors=\"pt\")\n",
    "# inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "# # Extract audio embeddings\n",
    "# with torch.no_grad():\n",
    "#     outputs = model.get_audio_features(**inputs)  # shape: [batch, feature_dim]\n",
    "\n",
    "# audio_embed = outputs  # shape: [1, 512]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load AudioLDM: %s audioldm-s-full-v2\n",
      "DiffusionWrapper has 185.04 M params.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/v2a-mapper/lib/python3.11/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "/home/ubuntu/miniconda3/envs/v2a-mapper/lib/python3.11/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/vocab.json HTTP/1.1\" 200 0\n",
      "INFO:root:Loading HTSAT-tiny model config.\n",
      "/home/ubuntu/miniconda3/envs/v2a-mapper/lib/python3.11/site-packages/torchlibrosa/stft.py:193: FutureWarning: Pass size=1024 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  fft_window = librosa.util.pad_center(fft_window, n_fft)\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "MODEL_NAME = \"audioldm-s-full-v2\"\n",
    "audioldm=build_model(model_name=MODEL_NAME)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def image2audio(file_name, images, duration=10, guidance_scale=2.5, random_seed=42, n_candidates=3):\n",
    "    waveform = clap_to_audio(\n",
    "        latent_diffusion=audioldm,\n",
    "        clap_feat=images,\n",
    "        seed=random_seed,\n",
    "        duration=duration,\n",
    "        guidance_scale=guidance_scale,\n",
    "        n_candidate_gen_per_text=int(n_candidates),\n",
    "    )  # [bs, 1, samples]\n",
    "\n",
    "    for i, wave in enumerate(waveform):\n",
    "        filename = f\"../output/{file_name}.wav\"\n",
    "        sf.write(filename, wave[0], 16000, 'PCM_16') \n",
    "\n",
    "    return waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch shape:  [tensor([[ 6.1245e-03,  1.6069e-02,  5.5601e-02, -6.2583e-02, -5.0328e-02,\n",
      "          4.1050e-02,  2.2514e-02, -2.7365e-02,  1.3244e-02,  2.7520e-02,\n",
      "         -2.5884e-02, -4.3049e-02,  4.5110e-02,  7.9565e-02,  3.4114e-02,\n",
      "         -6.2841e-03,  5.0500e-03,  1.8683e-02,  2.3052e-02, -3.1869e-02,\n",
      "          3.4993e-03,  2.1129e-02, -1.9924e-02,  3.6578e-03,  2.5397e-02,\n",
      "         -5.9745e-02, -7.1013e-02,  1.3497e-02, -2.6585e-03,  6.6438e-02,\n",
      "          2.5386e-02,  6.4305e-03, -3.0029e-02, -3.7125e-02,  5.4214e-02,\n",
      "          3.8048e-02, -6.9573e-02, -6.5653e-02,  1.7121e-02,  3.6397e-02,\n",
      "          3.4468e-02, -4.2671e-02, -4.7466e-03,  2.1016e-02, -6.6543e-02,\n",
      "         -2.0464e-02, -3.7350e-02, -2.8601e-02, -3.7733e-02,  2.7174e-02,\n",
      "          1.1792e-03,  4.0551e-02, -3.2220e-02, -8.6845e-02, -1.1159e-02,\n",
      "         -2.7319e-02,  8.0420e-02, -1.9781e-02, -1.8699e-03, -4.4303e-02,\n",
      "         -1.4986e-02, -9.9876e-03,  5.8313e-02,  7.5958e-02, -1.4166e-02,\n",
      "          2.2849e-02, -4.5340e-02,  3.2655e-02, -5.7856e-03, -2.2854e-02,\n",
      "          3.2924e-02, -4.1128e-02, -9.9197e-03, -6.2035e-02,  6.7907e-02,\n",
      "          1.3126e-02, -7.1476e-02, -1.5431e-02,  1.4407e-02, -1.4348e-02,\n",
      "          1.4977e-02, -1.8650e-02, -1.1083e-02,  1.0119e-02, -9.0725e-02,\n",
      "          6.8961e-02,  7.0937e-03, -2.6398e-02,  1.7523e-02,  7.9483e-02,\n",
      "          2.7800e-02,  2.4447e-02, -3.3177e-02, -2.1096e-02,  3.3716e-02,\n",
      "          8.9158e-02,  2.6347e-02,  4.0986e-02,  4.7211e-02, -4.9134e-03,\n",
      "          1.8567e-02,  3.0245e-02, -5.5873e-02,  2.4435e-02,  6.9209e-02,\n",
      "          2.5821e-02, -5.3750e-03, -3.8593e-02, -8.2710e-03, -9.9414e-03,\n",
      "          2.4613e-02,  3.4086e-02,  5.9379e-03,  1.1905e-03,  4.7455e-02,\n",
      "         -2.5680e-02,  3.4858e-02,  1.8535e-02, -2.3095e-02,  4.8930e-02,\n",
      "         -6.0352e-02,  2.1697e-02, -1.8150e-02,  2.5905e-02,  9.7510e-02,\n",
      "          1.8985e-02,  5.6027e-02, -1.6542e-02, -3.4853e-02, -8.1226e-02,\n",
      "          2.1387e-02, -3.8545e-03, -3.3661e-02,  1.3789e-02,  2.6797e-02,\n",
      "          1.3261e-02,  7.0384e-02, -5.9830e-02, -6.1570e-02,  2.2080e-02,\n",
      "         -6.7773e-02,  8.7767e-02, -2.4273e-02, -5.5713e-02,  4.5751e-02,\n",
      "         -5.7310e-02, -1.4976e-02, -6.2703e-02,  1.2329e-03,  1.0548e-02,\n",
      "         -4.2416e-02,  6.0140e-02, -2.4513e-02, -8.1367e-02,  5.1918e-02,\n",
      "          3.0060e-02, -1.5326e-02, -7.3174e-02, -3.5367e-02, -6.1913e-02,\n",
      "          5.7766e-03, -1.5842e-02, -6.9551e-02, -5.8500e-02, -1.7654e-02,\n",
      "         -3.9700e-02,  3.2079e-02,  8.6348e-02,  3.2268e-02, -2.7717e-02,\n",
      "         -6.8901e-03,  4.1287e-02, -5.5682e-02,  1.7552e-02,  2.9140e-02,\n",
      "          4.1073e-02,  5.5026e-02, -8.1425e-03,  6.9360e-03,  1.1572e-02,\n",
      "          2.5655e-02,  3.7738e-02,  7.0326e-03,  6.0269e-02, -4.2763e-02,\n",
      "          1.4089e-02,  7.5173e-03,  4.3513e-02, -1.2183e-02,  6.5351e-02,\n",
      "         -7.5896e-02, -7.2175e-02,  9.1443e-03,  1.7739e-02,  6.1839e-02,\n",
      "          5.3442e-02,  8.3226e-02, -3.0785e-02, -1.8440e-02,  4.7106e-02,\n",
      "          4.4251e-02, -1.2675e-03, -2.0886e-02, -7.0373e-03, -1.8694e-02,\n",
      "          3.6422e-02, -3.6368e-02,  3.0762e-02, -3.9598e-02, -1.2169e-02,\n",
      "          1.9706e-02, -3.2754e-02,  7.4239e-02,  4.3762e-02,  2.1488e-02,\n",
      "         -4.0607e-02,  6.1594e-03, -9.6732e-03, -9.7554e-03,  3.8623e-02,\n",
      "          5.2364e-02, -1.9348e-02,  1.5790e-02, -2.2250e-02, -7.0934e-02,\n",
      "          8.4007e-02, -4.0021e-02,  3.9689e-02,  2.9442e-02, -6.4930e-04,\n",
      "         -3.4869e-02,  1.4742e-02, -6.9373e-03, -1.4885e-02,  5.0496e-02,\n",
      "         -1.0603e-01, -3.7106e-02,  5.2070e-03,  1.3762e-02,  7.3703e-03,\n",
      "          2.5898e-02,  2.4931e-02, -3.7710e-02, -7.4194e-03, -5.0997e-02,\n",
      "          3.1833e-02,  5.9243e-02,  7.3984e-04, -1.9819e-02, -2.0736e-02,\n",
      "          1.5187e-02, -1.9731e-02, -7.7614e-04, -1.1498e-02, -2.6484e-02,\n",
      "         -2.4780e-02,  5.6880e-02,  6.6176e-02,  4.2637e-03,  3.4356e-03,\n",
      "         -3.9669e-02,  2.5663e-02, -3.3586e-02,  1.1640e-01, -3.0248e-02,\n",
      "         -1.2779e-02,  4.3351e-02, -5.8779e-02, -2.9965e-02, -1.1549e-01,\n",
      "         -2.9411e-02, -9.9557e-03, -2.5335e-02,  3.1312e-02, -2.9837e-02,\n",
      "         -5.5419e-03,  6.7420e-03, -4.1386e-02,  8.1140e-02, -3.8637e-02,\n",
      "          1.5462e-02, -8.2324e-02, -1.1189e-02, -2.7848e-03, -3.4198e-02,\n",
      "          4.4577e-02,  5.3233e-03,  7.1741e-02,  1.9941e-02,  5.5435e-03,\n",
      "         -1.7195e-04,  1.0097e-01,  6.1932e-02,  2.8675e-02,  2.7303e-02,\n",
      "         -6.6224e-02,  3.5611e-02,  2.2971e-02, -2.8462e-02,  6.3242e-02,\n",
      "          4.5992e-02, -3.8379e-02, -4.7602e-02,  5.0465e-02, -8.9315e-02,\n",
      "          5.8766e-03,  6.1480e-02,  6.3512e-02, -4.1970e-02,  2.4944e-02,\n",
      "          9.2729e-02,  1.4988e-01, -2.7973e-02,  1.4085e-03, -1.0146e-01,\n",
      "          7.1051e-02, -7.0738e-03, -4.2615e-02, -6.7891e-02,  1.3653e-02,\n",
      "          7.5885e-02,  9.8427e-03, -6.6290e-03,  1.7880e-02,  8.8528e-03,\n",
      "         -7.2971e-02, -1.7258e-02, -2.2963e-02, -3.3012e-03,  1.1165e-02,\n",
      "          5.8850e-05,  6.9002e-03,  6.1351e-02,  5.5851e-02, -1.3187e-02,\n",
      "          7.5097e-03, -6.5051e-02,  3.2927e-02, -2.4642e-02, -5.9581e-03,\n",
      "          6.0466e-02,  5.6474e-02,  6.6719e-03, -1.3253e-02,  6.6748e-02,\n",
      "         -2.2457e-02, -3.5472e-02, -7.2690e-02,  6.9658e-02, -8.4992e-02,\n",
      "         -1.2132e-02,  3.9129e-02,  3.2458e-02,  4.0249e-02,  6.8585e-02,\n",
      "          1.5166e-02,  5.9448e-02,  2.6765e-03,  1.0149e-02,  7.3780e-02,\n",
      "          1.3048e-02,  1.4849e-02,  8.2102e-02,  7.0249e-02, -5.0442e-02,\n",
      "          5.1238e-02,  6.3553e-03, -3.9378e-02,  2.0093e-02,  3.5316e-02,\n",
      "          4.5924e-02,  4.2255e-02, -1.0693e-02, -2.3790e-02, -2.5319e-02,\n",
      "         -2.2578e-02,  4.2062e-02,  3.0249e-02,  3.1576e-02,  4.9283e-02,\n",
      "          4.5913e-02, -2.4761e-02,  1.9558e-03, -3.2586e-02, -2.2133e-02,\n",
      "         -1.0871e-02,  5.2160e-02, -1.8923e-02,  9.9754e-03, -4.7242e-03,\n",
      "         -4.0695e-02,  3.6345e-02,  1.0307e-01, -2.1219e-03, -7.7716e-02,\n",
      "          1.4909e-02,  2.0771e-02,  3.7596e-02,  2.3009e-02, -1.0314e-02,\n",
      "          3.3348e-03, -1.8397e-02, -2.7588e-02,  1.8595e-02, -1.1386e-01,\n",
      "          4.8417e-02, -4.9010e-02,  6.4228e-02, -2.9263e-02, -3.0714e-02,\n",
      "          1.0088e-01, -1.2216e-03,  7.1489e-02,  2.2239e-03, -7.4926e-02,\n",
      "         -3.6042e-03,  3.3268e-02,  5.2928e-02,  2.4342e-02, -9.9666e-04,\n",
      "         -1.7700e-02, -6.1250e-03, -5.8666e-03, -4.2795e-02, -3.3530e-02,\n",
      "          2.4968e-02, -1.9410e-02,  4.3944e-02, -2.6671e-02, -2.4785e-02,\n",
      "         -4.1728e-02,  9.0492e-03, -3.1673e-03,  4.8355e-02, -1.1422e-02,\n",
      "          2.9468e-02,  2.3375e-03, -6.3088e-02,  4.3475e-02,  5.1437e-02,\n",
      "         -3.9424e-02,  1.1339e-02,  1.9229e-02,  8.3237e-02, -4.6138e-02,\n",
      "          1.2530e-03,  3.1809e-02,  1.1059e-02, -1.1875e-02, -1.0353e-03,\n",
      "          4.1538e-02, -2.9345e-02, -3.8589e-02,  3.8909e-03, -1.2625e-02,\n",
      "          4.2393e-02,  1.2112e-02,  3.3888e-02,  8.5425e-02, -1.3430e-01,\n",
      "         -1.9667e-02, -3.6625e-02, -9.5890e-02,  3.5582e-02, -1.1391e-02,\n",
      "          6.0953e-02, -6.8202e-02, -3.6740e-02,  7.7492e-02,  7.7006e-03,\n",
      "          1.0407e-01, -6.8813e-02, -7.5982e-02,  1.0220e-01, -4.0865e-02,\n",
      "         -8.3825e-02,  5.7991e-03, -1.1729e-02, -6.7069e-03, -4.5889e-02,\n",
      "          2.5790e-03,  3.3951e-02,  5.4669e-02, -1.2070e-02,  2.2999e-02,\n",
      "          4.2189e-04,  3.6969e-02, -3.5051e-02, -2.0651e-03,  1.4747e-02,\n",
      "          3.1025e-02, -1.7105e-02,  1.0759e-01,  5.6544e-03, -4.9747e-02,\n",
      "         -2.0254e-02, -9.3859e-03,  4.0623e-02,  2.1217e-02, -6.2213e-02,\n",
      "         -1.1873e-02,  4.4248e-02,  8.2298e-03,  8.0253e-03,  1.0903e-01,\n",
      "          7.9363e-02, -4.1871e-02,  1.4006e-02,  3.7225e-03, -3.2389e-02,\n",
      "         -2.4511e-02, -1.3219e-01]], device='cuda:0')]\n",
      "batch type:  <class 'list'>\n",
      "key: fbank\n",
      "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 1, 1024, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 200/200 [00:17<00:00, 11.56it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0.00018592, 0.00017485, 0.00016554, ..., 0.00018001,\n",
       "         0.00019949, 0.00014239]]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image2audio(\n",
    "    file_name=\"test\",\n",
    "    images=embed[0].to('cuda:0'),\n",
    "    duration=10,\n",
    "    guidance_scale=2.5,\n",
    "    random_seed=42,\n",
    "    n_candidates=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torchaudio\n",
    "# from transformers import AutoProcessor, AutoModel\n",
    "\n",
    "# # Load processor and model\n",
    "# processor = AutoProcessor.from_pretrained(\"laion/clap-htsat-unfused\")\n",
    "# model = AutoModel.from_pretrained(\"laion/clap-htsat-unfused\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extraction.vgg_sound import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:17 videos found in /home/ubuntu/project/subdata/video\n",
      "INFO:root:17 videos found in /home/ubuntu/project/subdata/train_subset.csv\n",
      "INFO:root:48 videos missing in /home/ubuntu/project/subdata/video\n"
     ]
    }
   ],
   "source": [
    "vgg_dataset = VGGSound(root=\"/home/ubuntu/project/subdata/video\",\n",
    "                           csv_path=\"/home/ubuntu/project/subdata/train_subset.csv\")\n",
    "\n",
    "data = vgg_dataset[0]\n",
    "\n",
    "# for data in vgg_dataset:\n",
    "#     # print(data['id'])\n",
    "#     # print(data['audio'].shape)\n",
    "#     # print(data['video'].shape)\n",
    "#     # print(data['video'].shape)\n",
    "#     # if data['id'] == '1msyXyqRvpY_000000.mp4':\n",
    "#     #     print(data['audio'].shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Image([[[[0.1294, 0.1294, 0.1333,  ..., 0.3961, 0.3059, 0.2784],\n",
       "         [0.1255, 0.1255, 0.1294,  ..., 0.3961, 0.3059, 0.2784],\n",
       "         [0.1255, 0.1255, 0.1294,  ..., 0.3843, 0.2980, 0.2706],\n",
       "         ...,\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.1020, 0.1020, 0.1020],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.1020, 0.1020, 0.1020],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0980, 0.1020, 0.1020]],\n",
       "\n",
       "        [[0.1451, 0.1451, 0.1490,  ..., 0.4353, 0.3569, 0.3255],\n",
       "         [0.1412, 0.1412, 0.1451,  ..., 0.4353, 0.3569, 0.3255],\n",
       "         [0.1412, 0.1412, 0.1451,  ..., 0.4314, 0.3569, 0.3255],\n",
       "         ...,\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0784, 0.0784, 0.0784],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0784, 0.0784, 0.0784],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0745, 0.0784, 0.0784]],\n",
       "\n",
       "        [[0.1765, 0.1765, 0.1804,  ..., 0.4275, 0.3725, 0.3765],\n",
       "         [0.1725, 0.1725, 0.1765,  ..., 0.4275, 0.3725, 0.3765],\n",
       "         [0.1686, 0.1686, 0.1765,  ..., 0.4118, 0.3647, 0.3686],\n",
       "         ...,\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.1137, 0.1137, 0.1137],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.1137, 0.1137, 0.1137],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.1098, 0.1137, 0.1137]]],\n",
       "\n",
       "\n",
       "       [[[0.1294, 0.1294, 0.1333,  ..., 0.3961, 0.3059, 0.2784],\n",
       "         [0.1255, 0.1255, 0.1294,  ..., 0.3961, 0.3059, 0.2784],\n",
       "         [0.1255, 0.1255, 0.1294,  ..., 0.3922, 0.3020, 0.2745],\n",
       "         ...,\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.1020, 0.1020, 0.1020],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.1020, 0.1020, 0.1020],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.1020, 0.1020, 0.1020]],\n",
       "\n",
       "        [[0.1451, 0.1451, 0.1490,  ..., 0.4353, 0.3569, 0.3255],\n",
       "         [0.1412, 0.1412, 0.1451,  ..., 0.4353, 0.3569, 0.3255],\n",
       "         [0.1412, 0.1412, 0.1451,  ..., 0.4353, 0.3569, 0.3255],\n",
       "         ...,\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0784, 0.0784, 0.0784],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0784, 0.0784, 0.0784],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0784, 0.0784, 0.0784]],\n",
       "\n",
       "        [[0.1765, 0.1765, 0.1804,  ..., 0.4275, 0.3725, 0.3765],\n",
       "         [0.1725, 0.1725, 0.1765,  ..., 0.4275, 0.3725, 0.3765],\n",
       "         [0.1686, 0.1686, 0.1765,  ..., 0.4196, 0.3686, 0.3725],\n",
       "         ...,\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.1137, 0.1137, 0.1137],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.1137, 0.1137, 0.1137],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.1137, 0.1137, 0.1137]]],\n",
       "\n",
       "\n",
       "       [[[0.1294, 0.1294, 0.1333,  ..., 0.3961, 0.3059, 0.2784],\n",
       "         [0.1255, 0.1255, 0.1294,  ..., 0.3961, 0.3059, 0.2784],\n",
       "         [0.1255, 0.1255, 0.1294,  ..., 0.3922, 0.3020, 0.2745],\n",
       "         ...,\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0980, 0.0980, 0.0980],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0980, 0.0980, 0.0980],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0980, 0.0980, 0.0980]],\n",
       "\n",
       "        [[0.1451, 0.1451, 0.1451,  ..., 0.4353, 0.3569, 0.3255],\n",
       "         [0.1412, 0.1412, 0.1451,  ..., 0.4353, 0.3569, 0.3255],\n",
       "         [0.1412, 0.1412, 0.1451,  ..., 0.4353, 0.3569, 0.3255],\n",
       "         ...,\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0745, 0.0745, 0.0745],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0745, 0.0745, 0.0745],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0745, 0.0745, 0.0745]],\n",
       "\n",
       "        [[0.1765, 0.1765, 0.1804,  ..., 0.4275, 0.3725, 0.3765],\n",
       "         [0.1725, 0.1725, 0.1765,  ..., 0.4275, 0.3725, 0.3765],\n",
       "         [0.1686, 0.1686, 0.1765,  ..., 0.4196, 0.3686, 0.3725],\n",
       "         ...,\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.1098, 0.1098, 0.1098],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.1098, 0.1098, 0.1098],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.1098, 0.1098, 0.1098]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[0.6039, 0.4941, 0.5176,  ..., 0.6667, 0.6863, 0.8353],\n",
       "         [0.7098, 0.5961, 0.6235,  ..., 0.6902, 0.7020, 0.8510],\n",
       "         [0.6902, 0.5882, 0.5529,  ..., 0.7020, 0.7216, 0.8549],\n",
       "         ...,\n",
       "         [0.1765, 0.1765, 0.1725,  ..., 0.0431, 0.0314, 0.0431],\n",
       "         [0.1882, 0.1843, 0.1608,  ..., 0.0510, 0.0353, 0.0431],\n",
       "         [0.1882, 0.1882, 0.1686,  ..., 0.0510, 0.0353, 0.0431]],\n",
       "\n",
       "        [[0.6431, 0.5098, 0.5333,  ..., 0.4902, 0.5059, 0.7137],\n",
       "         [0.7529, 0.6118, 0.6353,  ..., 0.5137, 0.5137, 0.7373],\n",
       "         [0.7529, 0.6196, 0.5843,  ..., 0.5294, 0.5255, 0.7569],\n",
       "         ...,\n",
       "         [0.0353, 0.0353, 0.0314,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0471, 0.0431, 0.0235,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0471, 0.0471, 0.0275,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.6627, 0.5333, 0.5725,  ..., 0.4392, 0.5216, 0.8039],\n",
       "         [0.7725, 0.6353, 0.6784,  ..., 0.4588, 0.5373, 0.8275],\n",
       "         [0.7922, 0.6627, 0.6471,  ..., 0.4706, 0.5569, 0.8588],\n",
       "         ...,\n",
       "         [0.0157, 0.0157, 0.0118,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0275, 0.0235, 0.0078,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0275, 0.0275, 0.0118,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       "\n",
       "\n",
       "       [[[0.6039, 0.4941, 0.5176,  ..., 0.7255, 0.7373, 0.8275],\n",
       "         [0.7098, 0.5961, 0.6235,  ..., 0.7294, 0.7647, 0.8235],\n",
       "         [0.6902, 0.5882, 0.5529,  ..., 0.7333, 0.7843, 0.8078],\n",
       "         ...,\n",
       "         [0.5961, 0.5961, 0.5922,  ..., 0.0431, 0.0314, 0.0431],\n",
       "         [0.5529, 0.5529, 0.5529,  ..., 0.0510, 0.0353, 0.0431],\n",
       "         [0.5255, 0.5255, 0.5216,  ..., 0.0510, 0.0353, 0.0431]],\n",
       "\n",
       "        [[0.6431, 0.5098, 0.5333,  ..., 0.5294, 0.5451, 0.7843],\n",
       "         [0.7529, 0.6118, 0.6353,  ..., 0.5294, 0.5725, 0.7882],\n",
       "         [0.7529, 0.6196, 0.5843,  ..., 0.5216, 0.6039, 0.7922],\n",
       "         ...,\n",
       "         [0.2745, 0.2745, 0.2745,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.2471, 0.2471, 0.2471,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.2196, 0.2196, 0.2196,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.6627, 0.5333, 0.5725,  ..., 0.4902, 0.6039, 0.8706],\n",
       "         [0.7725, 0.6353, 0.6784,  ..., 0.4902, 0.6392, 0.8902],\n",
       "         [0.7922, 0.6627, 0.6471,  ..., 0.4863, 0.6824, 0.8980],\n",
       "         ...,\n",
       "         [0.2392, 0.2392, 0.2431,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.2196, 0.2196, 0.2196,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.1922, 0.1922, 0.1961,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       "\n",
       "\n",
       "       [[[0.6039, 0.4941, 0.5176,  ..., 0.7255, 0.7373, 0.8314],\n",
       "         [0.7098, 0.5961, 0.6235,  ..., 0.7294, 0.7686, 0.8235],\n",
       "         [0.6902, 0.5882, 0.5529,  ..., 0.7373, 0.7882, 0.8039],\n",
       "         ...,\n",
       "         [0.4078, 0.4039, 0.4039,  ..., 0.0431, 0.0314, 0.0431],\n",
       "         [0.3294, 0.3216, 0.3216,  ..., 0.0510, 0.0353, 0.0431],\n",
       "         [0.2941, 0.3020, 0.2745,  ..., 0.0510, 0.0353, 0.0431]],\n",
       "\n",
       "        [[0.6431, 0.5098, 0.5333,  ..., 0.5294, 0.5490, 0.7843],\n",
       "         [0.7529, 0.6118, 0.6353,  ..., 0.5294, 0.5765, 0.7882],\n",
       "         [0.7529, 0.6196, 0.5843,  ..., 0.5176, 0.6118, 0.7922],\n",
       "         ...,\n",
       "         [0.1294, 0.1216, 0.1294,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0863, 0.0745, 0.0863,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0549, 0.0627, 0.0392,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.6627, 0.5333, 0.5725,  ..., 0.4902, 0.6078, 0.8706],\n",
       "         [0.7725, 0.6353, 0.6784,  ..., 0.4902, 0.6431, 0.8902],\n",
       "         [0.7922, 0.6627, 0.6471,  ..., 0.4863, 0.6863, 0.8980],\n",
       "         ...,\n",
       "         [0.0941, 0.0941, 0.1255,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0431, 0.0431, 0.0706,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0118, 0.0275, 0.0275,  ..., 0.0000, 0.0000, 0.0000]]]], )"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['clip_video']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# # Load audio file\n",
    "# audio_path = \"input_audio.wav\"\n",
    "# waveform, sr = torchaudio.load(audio_path)\n",
    "waveform = data['audio']\n",
    "\n",
    "# Resample to 48kHz (required by CLAP)\n",
    "# if sr != 48000:\n",
    "# resampler = torchaudio.transforms.Resample(16000, 48000)\n",
    "# waveform = resampler(waveform)\n",
    "\n",
    "# Convert to mono\n",
    "# if waveform.shape[0] > 1:\n",
    "#     waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "\n",
    "# Prepare inputs for the model\n",
    "inputs = processor(audios=waveform, sampling_rate=48000, return_tensors=\"pt\")\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "# Extract audio embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model.get_audio_features(**inputs)  # shape: [batch, feature_dim]\n",
    "\n",
    "audio_embed = outputs  # shape: [1, 512]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/v2a-mapper/lib/python3.11/site-packages/torchlibrosa/stft.py:193: FutureWarning: Pass size=1024 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  fft_window = librosa.util.pad_center(fft_window, n_fft)\n",
      "/home/ubuntu/miniconda3/envs/v2a-mapper/lib/python3.11/site-packages/torch/functional.py:539: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:3637.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "UnpicklingError",
     "evalue": "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([scalar])` or the `torch.serialization.safe_globals([scalar])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnpicklingError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (x * \u001b[32m32767.\u001b[39m).astype(np.int16)\n\u001b[32m     10\u001b[39m model = laion_clap.CLAP_Module(enable_fusion=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_ckpt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# download the default pretrained checkpoint.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/v2a-mapper/lib/python3.11/site-packages/laion_clap/hook.py:114\u001b[39m, in \u001b[36mCLAP_Module.load_ckpt\u001b[39m\u001b[34m(self, ckpt, model_id, verbose)\u001b[39m\n\u001b[32m    112\u001b[39m         logging.info(\u001b[33m'\u001b[39m\u001b[33mDownload completed!\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    113\u001b[39m logging.info(\u001b[33m'\u001b[39m\u001b[33mLoad Checkpoint...\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m ckpt = \u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckpt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_params\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[38;5;28mself\u001b[39m.model.load_state_dict(ckpt)\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/v2a-mapper/lib/python3.11/site-packages/laion_clap/clap_module/factory.py:54\u001b[39m, in \u001b[36mload_state_dict\u001b[39m\u001b[34m(checkpoint_path, map_location, skip_params)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_state_dict\u001b[39m(checkpoint_path: \u001b[38;5;28mstr\u001b[39m, map_location=\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m, skip_params=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     checkpoint = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(checkpoint, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mstate_dict\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m checkpoint:\n\u001b[32m     56\u001b[39m         state_dict = checkpoint[\u001b[33m\"\u001b[39m\u001b[33mstate_dict\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/v2a-mapper/lib/python3.11/site-packages/torch/serialization.py:1470\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1462\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[32m   1463\u001b[39m                     opened_zipfile,\n\u001b[32m   1464\u001b[39m                     map_location,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1467\u001b[39m                     **pickle_load_args,\n\u001b[32m   1468\u001b[39m                 )\n\u001b[32m   1469\u001b[39m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle.UnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m1470\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle.UnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1471\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[32m   1472\u001b[39m             opened_zipfile,\n\u001b[32m   1473\u001b[39m             map_location,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1476\u001b[39m             **pickle_load_args,\n\u001b[32m   1477\u001b[39m         )\n\u001b[32m   1478\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n",
      "\u001b[31mUnpicklingError\u001b[39m: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([scalar])` or the `torch.serialization.safe_globals([scalar])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html."
     ]
    }
   ],
   "source": [
    "\n",
    "# # quantization\n",
    "# def int16_to_float32(x):\n",
    "#     return (x / 32767.0).astype(np.float32)\n",
    "\n",
    "\n",
    "# def float32_to_int16(x):\n",
    "#     x = np.clip(x, a_min=-1., a_max=1.)\n",
    "#     return (x * 32767.).astype(np.int16)\n",
    "\n",
    "# model = laion_clap.CLAP_Module(enable_fusion=False)\n",
    "# model.load_ckpt() # download the default pretrained checkpoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Image([[[[0.1294, 0.1294, 0.1333,  ..., 0.3961, 0.3059, 0.2784],\n",
       "         [0.1255, 0.1255, 0.1294,  ..., 0.3961, 0.3059, 0.2784],\n",
       "         [0.1255, 0.1255, 0.1294,  ..., 0.3843, 0.2980, 0.2706],\n",
       "         ...,\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.1020, 0.1020, 0.1020],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.1020, 0.1020, 0.1020],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0980, 0.1020, 0.1020]],\n",
       "\n",
       "        [[0.1451, 0.1451, 0.1490,  ..., 0.4353, 0.3569, 0.3255],\n",
       "         [0.1412, 0.1412, 0.1451,  ..., 0.4353, 0.3569, 0.3255],\n",
       "         [0.1412, 0.1412, 0.1451,  ..., 0.4314, 0.3569, 0.3255],\n",
       "         ...,\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0784, 0.0784, 0.0784],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0784, 0.0784, 0.0784],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0745, 0.0784, 0.0784]],\n",
       "\n",
       "        [[0.1765, 0.1765, 0.1804,  ..., 0.4275, 0.3725, 0.3765],\n",
       "         [0.1725, 0.1725, 0.1765,  ..., 0.4275, 0.3725, 0.3765],\n",
       "         [0.1686, 0.1686, 0.1765,  ..., 0.4118, 0.3647, 0.3686],\n",
       "         ...,\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.1137, 0.1137, 0.1137],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.1137, 0.1137, 0.1137],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.1098, 0.1137, 0.1137]]],\n",
       "\n",
       "\n",
       "       [[[0.1294, 0.1294, 0.1333,  ..., 0.3961, 0.3059, 0.2784],\n",
       "         [0.1255, 0.1255, 0.1294,  ..., 0.3961, 0.3059, 0.2784],\n",
       "         [0.1255, 0.1255, 0.1294,  ..., 0.3922, 0.3020, 0.2745],\n",
       "         ...,\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.1020, 0.1020, 0.1020],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.1020, 0.1020, 0.1020],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.1020, 0.1020, 0.1020]],\n",
       "\n",
       "        [[0.1451, 0.1451, 0.1490,  ..., 0.4353, 0.3569, 0.3255],\n",
       "         [0.1412, 0.1412, 0.1451,  ..., 0.4353, 0.3569, 0.3255],\n",
       "         [0.1412, 0.1412, 0.1451,  ..., 0.4353, 0.3569, 0.3255],\n",
       "         ...,\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0784, 0.0784, 0.0784],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0784, 0.0784, 0.0784],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0784, 0.0784, 0.0784]],\n",
       "\n",
       "        [[0.1765, 0.1765, 0.1804,  ..., 0.4275, 0.3725, 0.3765],\n",
       "         [0.1725, 0.1725, 0.1765,  ..., 0.4275, 0.3725, 0.3765],\n",
       "         [0.1686, 0.1686, 0.1765,  ..., 0.4196, 0.3686, 0.3725],\n",
       "         ...,\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.1137, 0.1137, 0.1137],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.1137, 0.1137, 0.1137],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.1137, 0.1137, 0.1137]]],\n",
       "\n",
       "\n",
       "       [[[0.1294, 0.1294, 0.1333,  ..., 0.3961, 0.3059, 0.2784],\n",
       "         [0.1255, 0.1255, 0.1294,  ..., 0.3961, 0.3059, 0.2784],\n",
       "         [0.1255, 0.1255, 0.1294,  ..., 0.3922, 0.3020, 0.2745],\n",
       "         ...,\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0980, 0.0980, 0.0980],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0980, 0.0980, 0.0980],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0980, 0.0980, 0.0980]],\n",
       "\n",
       "        [[0.1451, 0.1451, 0.1451,  ..., 0.4353, 0.3569, 0.3255],\n",
       "         [0.1412, 0.1412, 0.1451,  ..., 0.4353, 0.3569, 0.3255],\n",
       "         [0.1412, 0.1412, 0.1451,  ..., 0.4353, 0.3569, 0.3255],\n",
       "         ...,\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0745, 0.0745, 0.0745],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0745, 0.0745, 0.0745],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0745, 0.0745, 0.0745]],\n",
       "\n",
       "        [[0.1765, 0.1765, 0.1804,  ..., 0.4275, 0.3725, 0.3765],\n",
       "         [0.1725, 0.1725, 0.1765,  ..., 0.4275, 0.3725, 0.3765],\n",
       "         [0.1686, 0.1686, 0.1765,  ..., 0.4196, 0.3686, 0.3725],\n",
       "         ...,\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.1098, 0.1098, 0.1098],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.1098, 0.1098, 0.1098],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.1098, 0.1098, 0.1098]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[0.6039, 0.4941, 0.5176,  ..., 0.6667, 0.6863, 0.8353],\n",
       "         [0.7098, 0.5961, 0.6235,  ..., 0.6902, 0.7020, 0.8510],\n",
       "         [0.6902, 0.5882, 0.5529,  ..., 0.7020, 0.7216, 0.8549],\n",
       "         ...,\n",
       "         [0.1765, 0.1765, 0.1725,  ..., 0.0431, 0.0314, 0.0431],\n",
       "         [0.1882, 0.1843, 0.1608,  ..., 0.0510, 0.0353, 0.0431],\n",
       "         [0.1882, 0.1882, 0.1686,  ..., 0.0510, 0.0353, 0.0431]],\n",
       "\n",
       "        [[0.6431, 0.5098, 0.5333,  ..., 0.4902, 0.5059, 0.7137],\n",
       "         [0.7529, 0.6118, 0.6353,  ..., 0.5137, 0.5137, 0.7373],\n",
       "         [0.7529, 0.6196, 0.5843,  ..., 0.5294, 0.5255, 0.7569],\n",
       "         ...,\n",
       "         [0.0353, 0.0353, 0.0314,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0471, 0.0431, 0.0235,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0471, 0.0471, 0.0275,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.6627, 0.5333, 0.5725,  ..., 0.4392, 0.5216, 0.8039],\n",
       "         [0.7725, 0.6353, 0.6784,  ..., 0.4588, 0.5373, 0.8275],\n",
       "         [0.7922, 0.6627, 0.6471,  ..., 0.4706, 0.5569, 0.8588],\n",
       "         ...,\n",
       "         [0.0157, 0.0157, 0.0118,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0275, 0.0235, 0.0078,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0275, 0.0275, 0.0118,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       "\n",
       "\n",
       "       [[[0.6039, 0.4941, 0.5176,  ..., 0.7255, 0.7373, 0.8275],\n",
       "         [0.7098, 0.5961, 0.6235,  ..., 0.7294, 0.7647, 0.8235],\n",
       "         [0.6902, 0.5882, 0.5529,  ..., 0.7333, 0.7843, 0.8078],\n",
       "         ...,\n",
       "         [0.5961, 0.5961, 0.5922,  ..., 0.0431, 0.0314, 0.0431],\n",
       "         [0.5529, 0.5529, 0.5529,  ..., 0.0510, 0.0353, 0.0431],\n",
       "         [0.5255, 0.5255, 0.5216,  ..., 0.0510, 0.0353, 0.0431]],\n",
       "\n",
       "        [[0.6431, 0.5098, 0.5333,  ..., 0.5294, 0.5451, 0.7843],\n",
       "         [0.7529, 0.6118, 0.6353,  ..., 0.5294, 0.5725, 0.7882],\n",
       "         [0.7529, 0.6196, 0.5843,  ..., 0.5216, 0.6039, 0.7922],\n",
       "         ...,\n",
       "         [0.2745, 0.2745, 0.2745,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.2471, 0.2471, 0.2471,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.2196, 0.2196, 0.2196,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.6627, 0.5333, 0.5725,  ..., 0.4902, 0.6039, 0.8706],\n",
       "         [0.7725, 0.6353, 0.6784,  ..., 0.4902, 0.6392, 0.8902],\n",
       "         [0.7922, 0.6627, 0.6471,  ..., 0.4863, 0.6824, 0.8980],\n",
       "         ...,\n",
       "         [0.2392, 0.2392, 0.2431,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.2196, 0.2196, 0.2196,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.1922, 0.1922, 0.1961,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       "\n",
       "\n",
       "       [[[0.6039, 0.4941, 0.5176,  ..., 0.7255, 0.7373, 0.8314],\n",
       "         [0.7098, 0.5961, 0.6235,  ..., 0.7294, 0.7686, 0.8235],\n",
       "         [0.6902, 0.5882, 0.5529,  ..., 0.7373, 0.7882, 0.8039],\n",
       "         ...,\n",
       "         [0.4078, 0.4039, 0.4039,  ..., 0.0431, 0.0314, 0.0431],\n",
       "         [0.3294, 0.3216, 0.3216,  ..., 0.0510, 0.0353, 0.0431],\n",
       "         [0.2941, 0.3020, 0.2745,  ..., 0.0510, 0.0353, 0.0431]],\n",
       "\n",
       "        [[0.6431, 0.5098, 0.5333,  ..., 0.5294, 0.5490, 0.7843],\n",
       "         [0.7529, 0.6118, 0.6353,  ..., 0.5294, 0.5765, 0.7882],\n",
       "         [0.7529, 0.6196, 0.5843,  ..., 0.5176, 0.6118, 0.7922],\n",
       "         ...,\n",
       "         [0.1294, 0.1216, 0.1294,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0863, 0.0745, 0.0863,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0549, 0.0627, 0.0392,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.6627, 0.5333, 0.5725,  ..., 0.4902, 0.6078, 0.8706],\n",
       "         [0.7725, 0.6353, 0.6784,  ..., 0.4902, 0.6431, 0.8902],\n",
       "         [0.7922, 0.6627, 0.6471,  ..., 0.4863, 0.6863, 0.8980],\n",
       "         ...,\n",
       "         [0.0941, 0.0941, 0.1255,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0431, 0.0431, 0.0706,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0118, 0.0275, 0.0275,  ..., 0.0000, 0.0000, 0.0000]]]], )"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['clip_video']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = data['clip_video'][0, :, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Image([[[[0.1294, 0.1294, 0.1333,  ..., 0.3961, 0.3059, 0.2784],\n",
       "         [0.1255, 0.1255, 0.1294,  ..., 0.3961, 0.3059, 0.2784],\n",
       "         [0.1255, 0.1255, 0.1294,  ..., 0.3843, 0.2980, 0.2706],\n",
       "         ...,\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.1020, 0.1020, 0.1020],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.1020, 0.1020, 0.1020],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0980, 0.1020, 0.1020]],\n",
       "\n",
       "        [[0.1451, 0.1451, 0.1490,  ..., 0.4353, 0.3569, 0.3255],\n",
       "         [0.1412, 0.1412, 0.1451,  ..., 0.4353, 0.3569, 0.3255],\n",
       "         [0.1412, 0.1412, 0.1451,  ..., 0.4314, 0.3569, 0.3255],\n",
       "         ...,\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0784, 0.0784, 0.0784],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0784, 0.0784, 0.0784],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0745, 0.0784, 0.0784]],\n",
       "\n",
       "        [[0.1765, 0.1765, 0.1804,  ..., 0.4275, 0.3725, 0.3765],\n",
       "         [0.1725, 0.1725, 0.1765,  ..., 0.4275, 0.3725, 0.3765],\n",
       "         [0.1686, 0.1686, 0.1765,  ..., 0.4118, 0.3647, 0.3686],\n",
       "         ...,\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.1137, 0.1137, 0.1137],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.1137, 0.1137, 0.1137],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.1098, 0.1137, 0.1137]]],\n",
       "\n",
       "\n",
       "       [[[0.1294, 0.1294, 0.1333,  ..., 0.3961, 0.3059, 0.2784],\n",
       "         [0.1255, 0.1255, 0.1294,  ..., 0.3961, 0.3059, 0.2784],\n",
       "         [0.1255, 0.1255, 0.1294,  ..., 0.3922, 0.3020, 0.2745],\n",
       "         ...,\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.1020, 0.1020, 0.1020],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.1020, 0.1020, 0.1020],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.1020, 0.1020, 0.1020]],\n",
       "\n",
       "        [[0.1451, 0.1451, 0.1490,  ..., 0.4353, 0.3569, 0.3255],\n",
       "         [0.1412, 0.1412, 0.1451,  ..., 0.4353, 0.3569, 0.3255],\n",
       "         [0.1412, 0.1412, 0.1451,  ..., 0.4353, 0.3569, 0.3255],\n",
       "         ...,\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0784, 0.0784, 0.0784],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0784, 0.0784, 0.0784],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0784, 0.0784, 0.0784]],\n",
       "\n",
       "        [[0.1765, 0.1765, 0.1804,  ..., 0.4275, 0.3725, 0.3765],\n",
       "         [0.1725, 0.1725, 0.1765,  ..., 0.4275, 0.3725, 0.3765],\n",
       "         [0.1686, 0.1686, 0.1765,  ..., 0.4196, 0.3686, 0.3725],\n",
       "         ...,\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.1137, 0.1137, 0.1137],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.1137, 0.1137, 0.1137],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.1137, 0.1137, 0.1137]]],\n",
       "\n",
       "\n",
       "       [[[0.1294, 0.1294, 0.1333,  ..., 0.3961, 0.3059, 0.2784],\n",
       "         [0.1255, 0.1255, 0.1294,  ..., 0.3961, 0.3059, 0.2784],\n",
       "         [0.1255, 0.1255, 0.1294,  ..., 0.3922, 0.3020, 0.2745],\n",
       "         ...,\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0980, 0.0980, 0.0980],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0980, 0.0980, 0.0980],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0980, 0.0980, 0.0980]],\n",
       "\n",
       "        [[0.1451, 0.1451, 0.1451,  ..., 0.4353, 0.3569, 0.3255],\n",
       "         [0.1412, 0.1412, 0.1451,  ..., 0.4353, 0.3569, 0.3255],\n",
       "         [0.1412, 0.1412, 0.1451,  ..., 0.4353, 0.3569, 0.3255],\n",
       "         ...,\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0745, 0.0745, 0.0745],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0745, 0.0745, 0.0745],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0745, 0.0745, 0.0745]],\n",
       "\n",
       "        [[0.1765, 0.1765, 0.1804,  ..., 0.4275, 0.3725, 0.3765],\n",
       "         [0.1725, 0.1725, 0.1765,  ..., 0.4275, 0.3725, 0.3765],\n",
       "         [0.1686, 0.1686, 0.1765,  ..., 0.4196, 0.3686, 0.3725],\n",
       "         ...,\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.1098, 0.1098, 0.1098],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.1098, 0.1098, 0.1098],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.1098, 0.1098, 0.1098]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[0.6039, 0.4941, 0.5176,  ..., 0.6667, 0.6863, 0.8353],\n",
       "         [0.7098, 0.5961, 0.6235,  ..., 0.6902, 0.7020, 0.8510],\n",
       "         [0.6902, 0.5882, 0.5529,  ..., 0.7020, 0.7216, 0.8549],\n",
       "         ...,\n",
       "         [0.1765, 0.1765, 0.1725,  ..., 0.0431, 0.0314, 0.0431],\n",
       "         [0.1882, 0.1843, 0.1608,  ..., 0.0510, 0.0353, 0.0431],\n",
       "         [0.1882, 0.1882, 0.1686,  ..., 0.0510, 0.0353, 0.0431]],\n",
       "\n",
       "        [[0.6431, 0.5098, 0.5333,  ..., 0.4902, 0.5059, 0.7137],\n",
       "         [0.7529, 0.6118, 0.6353,  ..., 0.5137, 0.5137, 0.7373],\n",
       "         [0.7529, 0.6196, 0.5843,  ..., 0.5294, 0.5255, 0.7569],\n",
       "         ...,\n",
       "         [0.0353, 0.0353, 0.0314,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0471, 0.0431, 0.0235,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0471, 0.0471, 0.0275,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.6627, 0.5333, 0.5725,  ..., 0.4392, 0.5216, 0.8039],\n",
       "         [0.7725, 0.6353, 0.6784,  ..., 0.4588, 0.5373, 0.8275],\n",
       "         [0.7922, 0.6627, 0.6471,  ..., 0.4706, 0.5569, 0.8588],\n",
       "         ...,\n",
       "         [0.0157, 0.0157, 0.0118,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0275, 0.0235, 0.0078,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0275, 0.0275, 0.0118,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       "\n",
       "\n",
       "       [[[0.6039, 0.4941, 0.5176,  ..., 0.7255, 0.7373, 0.8275],\n",
       "         [0.7098, 0.5961, 0.6235,  ..., 0.7294, 0.7647, 0.8235],\n",
       "         [0.6902, 0.5882, 0.5529,  ..., 0.7333, 0.7843, 0.8078],\n",
       "         ...,\n",
       "         [0.5961, 0.5961, 0.5922,  ..., 0.0431, 0.0314, 0.0431],\n",
       "         [0.5529, 0.5529, 0.5529,  ..., 0.0510, 0.0353, 0.0431],\n",
       "         [0.5255, 0.5255, 0.5216,  ..., 0.0510, 0.0353, 0.0431]],\n",
       "\n",
       "        [[0.6431, 0.5098, 0.5333,  ..., 0.5294, 0.5451, 0.7843],\n",
       "         [0.7529, 0.6118, 0.6353,  ..., 0.5294, 0.5725, 0.7882],\n",
       "         [0.7529, 0.6196, 0.5843,  ..., 0.5216, 0.6039, 0.7922],\n",
       "         ...,\n",
       "         [0.2745, 0.2745, 0.2745,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.2471, 0.2471, 0.2471,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.2196, 0.2196, 0.2196,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.6627, 0.5333, 0.5725,  ..., 0.4902, 0.6039, 0.8706],\n",
       "         [0.7725, 0.6353, 0.6784,  ..., 0.4902, 0.6392, 0.8902],\n",
       "         [0.7922, 0.6627, 0.6471,  ..., 0.4863, 0.6824, 0.8980],\n",
       "         ...,\n",
       "         [0.2392, 0.2392, 0.2431,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.2196, 0.2196, 0.2196,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.1922, 0.1922, 0.1961,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       "\n",
       "\n",
       "       [[[0.6039, 0.4941, 0.5176,  ..., 0.7255, 0.7373, 0.8314],\n",
       "         [0.7098, 0.5961, 0.6235,  ..., 0.7294, 0.7686, 0.8235],\n",
       "         [0.6902, 0.5882, 0.5529,  ..., 0.7373, 0.7882, 0.8039],\n",
       "         ...,\n",
       "         [0.4078, 0.4039, 0.4039,  ..., 0.0431, 0.0314, 0.0431],\n",
       "         [0.3294, 0.3216, 0.3216,  ..., 0.0510, 0.0353, 0.0431],\n",
       "         [0.2941, 0.3020, 0.2745,  ..., 0.0510, 0.0353, 0.0431]],\n",
       "\n",
       "        [[0.6431, 0.5098, 0.5333,  ..., 0.5294, 0.5490, 0.7843],\n",
       "         [0.7529, 0.6118, 0.6353,  ..., 0.5294, 0.5765, 0.7882],\n",
       "         [0.7529, 0.6196, 0.5843,  ..., 0.5176, 0.6118, 0.7922],\n",
       "         ...,\n",
       "         [0.1294, 0.1216, 0.1294,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0863, 0.0745, 0.0863,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0549, 0.0627, 0.0392,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.6627, 0.5333, 0.5725,  ..., 0.4902, 0.6078, 0.8706],\n",
       "         [0.7725, 0.6353, 0.6784,  ..., 0.4902, 0.6431, 0.8902],\n",
       "         [0.7922, 0.6627, 0.6471,  ..., 0.4863, 0.6863, 0.8980],\n",
       "         ...,\n",
       "         [0.0941, 0.0941, 0.1255,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0431, 0.0431, 0.0706,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0118, 0.0275, 0.0275,  ..., 0.0000, 0.0000, 0.0000]]]], )"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['clip_video']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 224, 224)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directly get audio embeddings from audio files\n",
    "audio_file = [\n",
    "    '/home/data/test_clap_short.wav',\n",
    "    '/home/data/test_clap_long.wav'\n",
    "]\n",
    "audio_embed = model.get_audio_embedding_from_filelist(x = audio_file, use_tensor=False)\n",
    "print(audio_embed[:,-20:])\n",
    "print(audio_embed.shape)\n",
    "\n",
    "# Get audio embeddings from audio data\n",
    "audio_data, _ = librosa.load('/home/data/test_clap_short.wav', sr=48000) # sample rate should be 48000\n",
    "audio_data = audio_data.reshape(1, -1) # Make it (1,T) or (N,T)\n",
    "audio_embed = model.get_audio_embedding_from_data(x = audio_data, use_tensor=False)\n",
    "print(audio_embed[:,-20:])\n",
    "print(audio_embed.shape)\n",
    "\n",
    "# Directly get audio embeddings from audio files, but return torch tensor\n",
    "audio_file = [\n",
    "    '/home/data/test_clap_short.wav',\n",
    "    '/home/data/test_clap_long.wav'\n",
    "]\n",
    "audio_embed = model.get_audio_embedding_from_filelist(x = audio_file, use_tensor=True)\n",
    "print(audio_embed[:,-20:])\n",
    "print(audio_embed.shape)\n",
    "\n",
    "# Get audio embeddings from audio data\n",
    "audio_data, _ = librosa.load('/home/data/test_clap_short.wav', sr=48000) # sample rate should be 48000\n",
    "audio_data = audio_data.reshape(1, -1) # Make it (1,T) or (N,T)\n",
    "audio_data = torch.from_numpy(int16_to_float32(float32_to_int16(audio_data))).float() # quantize before send it in to the model\n",
    "audio_embed = model.get_audio_embedding_from_data(x = audio_data, use_tensor=True)\n",
    "print(audio_embed[:,-20:])\n",
    "print(audio_embed.shape)\n",
    "\n",
    "# Get text embedings from texts:\n",
    "text_data = [\"I love the contrastive learning\", \"I love the pretrain model\"] \n",
    "text_embed = model.get_text_embedding(text_data)\n",
    "print(text_embed)\n",
    "print(text_embed.shape)\n",
    "\n",
    "# Get text embedings from texts, but return torch tensor:\n",
    "text_data = [\"I love the contrastive learning\", \"I love the pretrain model\"] \n",
    "text_embed = model.get_text_embedding(text_data, use_tensor=True)\n",
    "print(text_embed)\n",
    "print(text_embed.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audioldm import image_to_audio, build_model\n",
    "MODEL_NAME = \"audioldm-s-full-v2\"\n",
    "audioldm=build_model(model_name=MODEL_NAME)\n",
    "\n",
    "def image2audio(file_name, images, duration=10, guidance_scale=2.5, random_seed=42, n_candidates=3):\n",
    "    waveform = image_to_audio(\n",
    "        latent_diffusion=audioldm,\n",
    "        images=images,\n",
    "        seed=random_seed,\n",
    "        duration=duration,\n",
    "        guidance_scale=guidance_scale,\n",
    "        n_candidate_gen_per_text=int(n_candidates),\n",
    "    )  # [bs, 1, samples]\n",
    "\n",
    "    for i, wave in enumerate(waveform):\n",
    "        filename = f\"../output/{file_name}.wav\"\n",
    "        sf.write(filename, wave[0], 16000, 'PCM_16') \n",
    "\n",
    "    return waveform\n",
    "\n",
    "image2audio(\n",
    "    file_name=\"test\",\n",
    "    images=data['clip_video'],\n",
    "    duration=10,\n",
    "    guidance_scale=2.5,\n",
    "    random_seed=42,\n",
    "    n_candidates=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load AudioLDM: %s audioldm-s-full-v2\n",
      "DiffusionWrapper has 185.04 M params.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/v2a-mapper/lib/python3.11/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "/home/ubuntu/miniconda3/envs/v2a-mapper/lib/python3.11/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/vocab.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/vocab.json HTTP/1.1\" 200 0\n",
      "INFO:root:Loading HTSAT-tiny model config.\n",
      "/home/ubuntu/miniconda3/envs/v2a-mapper/lib/python3.11/site-packages/torchlibrosa/stft.py:193: FutureWarning: Pass size=1024 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  fft_window = librosa.util.pad_center(fft_window, n_fft)\n",
      "/home/ubuntu/miniconda3/envs/v2a-mapper/lib/python3.11/site-packages/torch/functional.py:539: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:3637.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"audioldm-s-full-v2\"\n",
    "audioldm=build_model(model_name=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image2audio(file_name, images, duration=10, guidance_scale=2.5, random_seed=42, n_candidates=3):\n",
    "    waveform = image_to_audio(\n",
    "        latent_diffusion=audioldm,\n",
    "        images=images,\n",
    "        seed=random_seed,\n",
    "        duration=duration,\n",
    "        guidance_scale=guidance_scale,\n",
    "        n_candidate_gen_per_text=int(n_candidates),\n",
    "    )  # [bs, 1, samples]\n",
    "\n",
    "    for i, wave in enumerate(waveform):\n",
    "        filename = f\"../output/{file_name}.wav\"\n",
    "        sf.write(filename, wave[0], 16000, 'PCM_16') \n",
    "\n",
    "    return waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate audio using image Image([[[[0.1294, 0.1294, 0.1333,  ..., 0.3961, 0.3059, 0.2784],\n",
      "         [0.1255, 0.1255, 0.1294,  ..., 0.3961, 0.3059, 0.2784],\n",
      "         [0.1255, 0.1255, 0.1294,  ..., 0.3843, 0.2980, 0.2706],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.1020, 0.1020, 0.1020],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.1020, 0.1020, 0.1020],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0980, 0.1020, 0.1020]],\n",
      "\n",
      "        [[0.1451, 0.1451, 0.1490,  ..., 0.4353, 0.3569, 0.3255],\n",
      "         [0.1412, 0.1412, 0.1451,  ..., 0.4353, 0.3569, 0.3255],\n",
      "         [0.1412, 0.1412, 0.1451,  ..., 0.4314, 0.3569, 0.3255],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0784, 0.0784, 0.0784],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0784, 0.0784, 0.0784],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0745, 0.0784, 0.0784]],\n",
      "\n",
      "        [[0.1765, 0.1765, 0.1804,  ..., 0.4275, 0.3725, 0.3765],\n",
      "         [0.1725, 0.1725, 0.1765,  ..., 0.4275, 0.3725, 0.3765],\n",
      "         [0.1686, 0.1686, 0.1765,  ..., 0.4118, 0.3647, 0.3686],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.1137, 0.1137, 0.1137],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.1137, 0.1137, 0.1137],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.1098, 0.1137, 0.1137]]],\n",
      "\n",
      "\n",
      "       [[[0.1294, 0.1294, 0.1333,  ..., 0.3961, 0.3059, 0.2784],\n",
      "         [0.1255, 0.1255, 0.1294,  ..., 0.3961, 0.3059, 0.2784],\n",
      "         [0.1255, 0.1255, 0.1294,  ..., 0.3922, 0.3020, 0.2745],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.1020, 0.1020, 0.1020],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.1020, 0.1020, 0.1020],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.1020, 0.1020, 0.1020]],\n",
      "\n",
      "        [[0.1451, 0.1451, 0.1490,  ..., 0.4353, 0.3569, 0.3255],\n",
      "         [0.1412, 0.1412, 0.1451,  ..., 0.4353, 0.3569, 0.3255],\n",
      "         [0.1412, 0.1412, 0.1451,  ..., 0.4353, 0.3569, 0.3255],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0784, 0.0784, 0.0784],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0784, 0.0784, 0.0784],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0784, 0.0784, 0.0784]],\n",
      "\n",
      "        [[0.1765, 0.1765, 0.1804,  ..., 0.4275, 0.3725, 0.3765],\n",
      "         [0.1725, 0.1725, 0.1765,  ..., 0.4275, 0.3725, 0.3765],\n",
      "         [0.1686, 0.1686, 0.1765,  ..., 0.4196, 0.3686, 0.3725],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.1137, 0.1137, 0.1137],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.1137, 0.1137, 0.1137],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.1137, 0.1137, 0.1137]]],\n",
      "\n",
      "\n",
      "       [[[0.1294, 0.1294, 0.1333,  ..., 0.3961, 0.3059, 0.2784],\n",
      "         [0.1255, 0.1255, 0.1294,  ..., 0.3961, 0.3059, 0.2784],\n",
      "         [0.1255, 0.1255, 0.1294,  ..., 0.3922, 0.3020, 0.2745],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0980, 0.0980, 0.0980],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0980, 0.0980, 0.0980],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0980, 0.0980, 0.0980]],\n",
      "\n",
      "        [[0.1451, 0.1451, 0.1451,  ..., 0.4353, 0.3569, 0.3255],\n",
      "         [0.1412, 0.1412, 0.1451,  ..., 0.4353, 0.3569, 0.3255],\n",
      "         [0.1412, 0.1412, 0.1451,  ..., 0.4353, 0.3569, 0.3255],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0745, 0.0745, 0.0745],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0745, 0.0745, 0.0745],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0745, 0.0745, 0.0745]],\n",
      "\n",
      "        [[0.1765, 0.1765, 0.1804,  ..., 0.4275, 0.3725, 0.3765],\n",
      "         [0.1725, 0.1725, 0.1765,  ..., 0.4275, 0.3725, 0.3765],\n",
      "         [0.1686, 0.1686, 0.1765,  ..., 0.4196, 0.3686, 0.3725],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.1098, 0.1098, 0.1098],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.1098, 0.1098, 0.1098],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.1098, 0.1098, 0.1098]]],\n",
      "\n",
      "\n",
      "       ...,\n",
      "\n",
      "\n",
      "       [[[0.6039, 0.4941, 0.5176,  ..., 0.6667, 0.6863, 0.8353],\n",
      "         [0.7098, 0.5961, 0.6235,  ..., 0.6902, 0.7020, 0.8510],\n",
      "         [0.6902, 0.5882, 0.5529,  ..., 0.7020, 0.7216, 0.8549],\n",
      "         ...,\n",
      "         [0.1765, 0.1765, 0.1725,  ..., 0.0431, 0.0314, 0.0431],\n",
      "         [0.1882, 0.1843, 0.1608,  ..., 0.0510, 0.0353, 0.0431],\n",
      "         [0.1882, 0.1882, 0.1686,  ..., 0.0510, 0.0353, 0.0431]],\n",
      "\n",
      "        [[0.6431, 0.5098, 0.5333,  ..., 0.4902, 0.5059, 0.7137],\n",
      "         [0.7529, 0.6118, 0.6353,  ..., 0.5137, 0.5137, 0.7373],\n",
      "         [0.7529, 0.6196, 0.5843,  ..., 0.5294, 0.5255, 0.7569],\n",
      "         ...,\n",
      "         [0.0353, 0.0353, 0.0314,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0471, 0.0431, 0.0235,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0471, 0.0471, 0.0275,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.6627, 0.5333, 0.5725,  ..., 0.4392, 0.5216, 0.8039],\n",
      "         [0.7725, 0.6353, 0.6784,  ..., 0.4588, 0.5373, 0.8275],\n",
      "         [0.7922, 0.6627, 0.6471,  ..., 0.4706, 0.5569, 0.8588],\n",
      "         ...,\n",
      "         [0.0157, 0.0157, 0.0118,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0275, 0.0235, 0.0078,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0275, 0.0275, 0.0118,  ..., 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "       [[[0.6039, 0.4941, 0.5176,  ..., 0.7255, 0.7373, 0.8275],\n",
      "         [0.7098, 0.5961, 0.6235,  ..., 0.7294, 0.7647, 0.8235],\n",
      "         [0.6902, 0.5882, 0.5529,  ..., 0.7333, 0.7843, 0.8078],\n",
      "         ...,\n",
      "         [0.5961, 0.5961, 0.5922,  ..., 0.0431, 0.0314, 0.0431],\n",
      "         [0.5529, 0.5529, 0.5529,  ..., 0.0510, 0.0353, 0.0431],\n",
      "         [0.5255, 0.5255, 0.5216,  ..., 0.0510, 0.0353, 0.0431]],\n",
      "\n",
      "        [[0.6431, 0.5098, 0.5333,  ..., 0.5294, 0.5451, 0.7843],\n",
      "         [0.7529, 0.6118, 0.6353,  ..., 0.5294, 0.5725, 0.7882],\n",
      "         [0.7529, 0.6196, 0.5843,  ..., 0.5216, 0.6039, 0.7922],\n",
      "         ...,\n",
      "         [0.2745, 0.2745, 0.2745,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.2471, 0.2471, 0.2471,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.2196, 0.2196, 0.2196,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.6627, 0.5333, 0.5725,  ..., 0.4902, 0.6039, 0.8706],\n",
      "         [0.7725, 0.6353, 0.6784,  ..., 0.4902, 0.6392, 0.8902],\n",
      "         [0.7922, 0.6627, 0.6471,  ..., 0.4863, 0.6824, 0.8980],\n",
      "         ...,\n",
      "         [0.2392, 0.2392, 0.2431,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.2196, 0.2196, 0.2196,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.1922, 0.1922, 0.1961,  ..., 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "       [[[0.6039, 0.4941, 0.5176,  ..., 0.7255, 0.7373, 0.8314],\n",
      "         [0.7098, 0.5961, 0.6235,  ..., 0.7294, 0.7686, 0.8235],\n",
      "         [0.6902, 0.5882, 0.5529,  ..., 0.7373, 0.7882, 0.8039],\n",
      "         ...,\n",
      "         [0.4078, 0.4039, 0.4039,  ..., 0.0431, 0.0314, 0.0431],\n",
      "         [0.3294, 0.3216, 0.3216,  ..., 0.0510, 0.0353, 0.0431],\n",
      "         [0.2941, 0.3020, 0.2745,  ..., 0.0510, 0.0353, 0.0431]],\n",
      "\n",
      "        [[0.6431, 0.5098, 0.5333,  ..., 0.5294, 0.5490, 0.7843],\n",
      "         [0.7529, 0.6118, 0.6353,  ..., 0.5294, 0.5765, 0.7882],\n",
      "         [0.7529, 0.6196, 0.5843,  ..., 0.5176, 0.6118, 0.7922],\n",
      "         ...,\n",
      "         [0.1294, 0.1216, 0.1294,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0863, 0.0745, 0.0863,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0549, 0.0627, 0.0392,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.6627, 0.5333, 0.5725,  ..., 0.4902, 0.6078, 0.8706],\n",
      "         [0.7725, 0.6353, 0.6784,  ..., 0.4902, 0.6431, 0.8902],\n",
      "         [0.7922, 0.6627, 0.6471,  ..., 0.4863, 0.6863, 0.8980],\n",
      "         ...,\n",
      "         [0.0941, 0.0941, 0.1255,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0431, 0.0431, 0.0706,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0118, 0.0275, 0.0275,  ..., 0.0000, 0.0000, 0.0000]]]], )\n",
      "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 1, 1024, 64])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Could not infer dtype of NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mimage2audio\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtest\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mclip_video\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\n\u001b[32m      8\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mimage2audio\u001b[39m\u001b[34m(file_name, images, duration, guidance_scale, random_seed, n_candidates)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mimage2audio\u001b[39m(file_name, images, duration=\u001b[32m10\u001b[39m, guidance_scale=\u001b[32m2.5\u001b[39m, random_seed=\u001b[32m42\u001b[39m, n_candidates=\u001b[32m3\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     waveform = \u001b[43mimage_to_audio\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlatent_diffusion\u001b[49m\u001b[43m=\u001b[49m\u001b[43maudioldm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m=\u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_candidate_gen_per_text\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [bs, 1, samples]\u001b[39;00m\n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, wave \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(waveform):\n\u001b[32m     12\u001b[39m         filename = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m../output/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.wav\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/v2a-mapper/audioldm/pipeline.py:131\u001b[39m, in \u001b[36mimage_to_audio\u001b[39m\u001b[34m(latent_diffusion, images, original_audio_file_path, seed, ddim_steps, duration, batchsize, guidance_scale, n_candidate_gen_per_text, config)\u001b[39m\n\u001b[32m    128\u001b[39m latent_diffusion = set_cond_image(latent_diffusion)\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m     waveform = \u001b[43mlatent_diffusion\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m        \u001b[49m\u001b[43munconditional_guidance_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m        \u001b[49m\u001b[43mddim_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mddim_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_candidate_gen_per_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_candidate_gen_per_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m        \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m=\u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m waveform\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/v2a-mapper/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/v2a-mapper/audioldm/ldm.py:652\u001b[39m, in \u001b[36mLatentDiffusion.generate_sample\u001b[39m\u001b[34m(self, batchs, ddim_steps, ddim_eta, x_T, n_candidate_gen_per_text, unconditional_guidance_scale, unconditional_conditioning, name, use_plms, save, **kwargs)\u001b[39m\n\u001b[32m    650\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ema_scope(\u001b[33m\"\u001b[39m\u001b[33mGenerate\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    651\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m batchs:\n\u001b[32m--> \u001b[39m\u001b[32m652\u001b[39m         z, c = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_input\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfirst_stage_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcond_key\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcond_stage_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreturn_first_stage_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[43m            \u001b[49m\u001b[43mforce_c_encode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreturn_original_cond\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    659\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    660\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    661\u001b[39m         image = \u001b[38;5;28msuper\u001b[39m().get_input(batch, \u001b[33m\"\u001b[39m\u001b[33mimage\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;66;03m# 从batch里面取出image\u001b[39;00m\n\u001b[32m    663\u001b[39m         \u001b[38;5;66;03m# Generate multiple samples\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/v2a-mapper/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/v2a-mapper/audioldm/ldm.py:197\u001b[39m, in \u001b[36mLatentDiffusion.get_input\u001b[39m\u001b[34m(self, batch, k, return_first_stage_encode, return_first_stage_outputs, force_c_encode, cond_key, return_original_cond, bs)\u001b[39m\n\u001b[32m    195\u001b[39m     clip = calculate_clip(xc)\n\u001b[32m    196\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m         c = v2a_mapper_model(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclip\u001b[49m\u001b[43m)\u001b[49m).to(\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m bs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    200\u001b[39m     c = c[:bs]\n",
      "\u001b[31mRuntimeError\u001b[39m: Could not infer dtype of NoneType"
     ]
    }
   ],
   "source": [
    "image2audio(\n",
    "    file_name=\"test\",\n",
    "    images=data['clip_video'],\n",
    "    duration=10,\n",
    "    guidance_scale=2.5,\n",
    "    random_seed=42,\n",
    "    n_candidates=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AutoencoderKL(\n",
       "  (encoder): Encoder(\n",
       "    (conv_in): Conv2d(1, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (down): ModuleList(\n",
       "      (0): Module(\n",
       "        (block): ModuleList(\n",
       "          (0-1): 2 x ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (attn): ModuleList()\n",
       "        (downsample): Downsample(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2))\n",
       "        )\n",
       "      )\n",
       "      (1): Module(\n",
       "        (block): ModuleList(\n",
       "          (0): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nin_shortcut): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (attn): ModuleList()\n",
       "        (downsample): Downsample(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))\n",
       "        )\n",
       "      )\n",
       "      (2): Module(\n",
       "        (block): ModuleList(\n",
       "          (0): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nin_shortcut): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (attn): ModuleList()\n",
       "      )\n",
       "    )\n",
       "    (mid): Module(\n",
       "      (block_1): ResnetBlock(\n",
       "        (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (attn_1): AttnBlock(\n",
       "        (norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (block_2): ResnetBlock(\n",
       "        (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (norm_out): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "    (conv_out): Conv2d(512, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (conv_in): Conv2d(8, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (mid): Module(\n",
       "      (block_1): ResnetBlock(\n",
       "        (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (attn_1): AttnBlock(\n",
       "        (norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (block_2): ResnetBlock(\n",
       "        (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (up): ModuleList(\n",
       "      (0): Module(\n",
       "        (block): ModuleList(\n",
       "          (0): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nin_shortcut): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1-2): 2 x ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (attn): ModuleList()\n",
       "      )\n",
       "      (1): Module(\n",
       "        (block): ModuleList(\n",
       "          (0): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nin_shortcut): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1-2): 2 x ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (attn): ModuleList()\n",
       "        (upsample): Upsample(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (2): Module(\n",
       "        (block): ModuleList(\n",
       "          (0-2): 3 x ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (attn): ModuleList()\n",
       "        (upsample): Upsample(\n",
       "          (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm_out): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "    (conv_out): Conv2d(128, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (quant_conv): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (post_quant_conv): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (vocoder): Generator(\n",
       "    (conv_pre): Conv1d(64, 1024, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "    (ups): ModuleList(\n",
       "      (0): ConvTranspose1d(1024, 512, kernel_size=(16,), stride=(5,), padding=(5,))\n",
       "      (1): ConvTranspose1d(512, 256, kernel_size=(16,), stride=(4,), padding=(6,))\n",
       "      (2): ConvTranspose1d(256, 128, kernel_size=(8,), stride=(2,), padding=(3,))\n",
       "      (3): ConvTranspose1d(128, 64, kernel_size=(4,), stride=(2,), padding=(1,))\n",
       "      (4): ConvTranspose1d(64, 32, kernel_size=(4,), stride=(2,), padding=(1,))\n",
       "    )\n",
       "    (resblocks): ModuleList(\n",
       "      (0): ResBlock(\n",
       "        (convs1): ModuleList(\n",
       "          (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
       "          (2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
       "        )\n",
       "        (convs2): ModuleList(\n",
       "          (0-2): 3 x Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        )\n",
       "      )\n",
       "      (1): ResBlock(\n",
       "        (convs1): ModuleList(\n",
       "          (0): Conv1d(512, 512, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "          (1): Conv1d(512, 512, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
       "          (2): Conv1d(512, 512, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
       "        )\n",
       "        (convs2): ModuleList(\n",
       "          (0-2): 3 x Conv1d(512, 512, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "        )\n",
       "      )\n",
       "      (2): ResBlock(\n",
       "        (convs1): ModuleList(\n",
       "          (0): Conv1d(512, 512, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "          (1): Conv1d(512, 512, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
       "          (2): Conv1d(512, 512, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
       "        )\n",
       "        (convs2): ModuleList(\n",
       "          (0-2): 3 x Conv1d(512, 512, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "        )\n",
       "      )\n",
       "      (3): ResBlock(\n",
       "        (convs1): ModuleList(\n",
       "          (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
       "          (2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
       "        )\n",
       "        (convs2): ModuleList(\n",
       "          (0-2): 3 x Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        )\n",
       "      )\n",
       "      (4): ResBlock(\n",
       "        (convs1): ModuleList(\n",
       "          (0): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "          (1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
       "          (2): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
       "        )\n",
       "        (convs2): ModuleList(\n",
       "          (0-2): 3 x Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "        )\n",
       "      )\n",
       "      (5): ResBlock(\n",
       "        (convs1): ModuleList(\n",
       "          (0): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "          (1): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
       "          (2): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
       "        )\n",
       "        (convs2): ModuleList(\n",
       "          (0-2): 3 x Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "        )\n",
       "      )\n",
       "      (6): ResBlock(\n",
       "        (convs1): ModuleList(\n",
       "          (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
       "          (2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
       "        )\n",
       "        (convs2): ModuleList(\n",
       "          (0-2): 3 x Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        )\n",
       "      )\n",
       "      (7): ResBlock(\n",
       "        (convs1): ModuleList(\n",
       "          (0): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "          (1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
       "          (2): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
       "        )\n",
       "        (convs2): ModuleList(\n",
       "          (0-2): 3 x Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "        )\n",
       "      )\n",
       "      (8): ResBlock(\n",
       "        (convs1): ModuleList(\n",
       "          (0): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "          (1): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
       "          (2): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
       "        )\n",
       "        (convs2): ModuleList(\n",
       "          (0-2): 3 x Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "        )\n",
       "      )\n",
       "      (9): ResBlock(\n",
       "        (convs1): ModuleList(\n",
       "          (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
       "          (2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
       "        )\n",
       "        (convs2): ModuleList(\n",
       "          (0-2): 3 x Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        )\n",
       "      )\n",
       "      (10): ResBlock(\n",
       "        (convs1): ModuleList(\n",
       "          (0): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "          (1): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
       "          (2): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
       "        )\n",
       "        (convs2): ModuleList(\n",
       "          (0-2): 3 x Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "        )\n",
       "      )\n",
       "      (11): ResBlock(\n",
       "        (convs1): ModuleList(\n",
       "          (0): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "          (1): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
       "          (2): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
       "        )\n",
       "        (convs2): ModuleList(\n",
       "          (0-2): 3 x Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "        )\n",
       "      )\n",
       "      (12): ResBlock(\n",
       "        (convs1): ModuleList(\n",
       "          (0): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (1): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
       "          (2): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
       "        )\n",
       "        (convs2): ModuleList(\n",
       "          (0-2): 3 x Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        )\n",
       "      )\n",
       "      (13): ResBlock(\n",
       "        (convs1): ModuleList(\n",
       "          (0): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "          (1): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
       "          (2): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
       "        )\n",
       "        (convs2): ModuleList(\n",
       "          (0-2): 3 x Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "        )\n",
       "      )\n",
       "      (14): ResBlock(\n",
       "        (convs1): ModuleList(\n",
       "          (0): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "          (1): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
       "          (2): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
       "        )\n",
       "        (convs2): ModuleList(\n",
       "          (0-2): 3 x Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (conv_post): Conv1d(32, 1, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audioldm.first_stage_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/vocab.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/vocab.json HTTP/1.1\" 200 0\n",
      "INFO:root:Loading HTSAT-tiny model config.\n",
      "/home/ubuntu/miniconda3/envs/v2a-mapper/lib/python3.11/site-packages/torchlibrosa/stft.py:193: FutureWarning: Pass size=1024 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  fft_window = librosa.util.pad_center(fft_window, n_fft)\n",
      "/home/ubuntu/miniconda3/envs/v2a-mapper/lib/python3.11/site-packages/torch/functional.py:539: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:3637.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO:root:Loading pretrained HTSAT-tiny-roberta weights (/home/ubuntu/project/v2a-mapper/pretrain/clap_htsat_tiny.pt).\n"
     ]
    }
   ],
   "source": [
    "from audioldm.clap.encoders import CLAPAudioEmbeddingClassifierFreev2\n",
    "CLAP = CLAPAudioEmbeddingClassifierFreev2(\n",
    "    key='waveform',\n",
    "    pretrained_path=\"/home/ubuntu/project/v2a-mapper/pretrain/clap_htsat_tiny.pt\",\n",
    "    sampling_rate=16000,\n",
    "    embed_mode=\"audio\",\n",
    "    amodel=\"HTSAT-tiny\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'wave_file_save_path': './output',\n",
       " 'id': {'version': 'v1',\n",
       "  'name': 'default',\n",
       "  'root': '/mnt/fast/nobackup/users/hl01486/projects/general_audio_generation/AudioLDM-python/config/default/latent_diffusion.yaml'},\n",
       " 'preprocessing': {'audio': {'sampling_rate': 16000, 'max_wav_value': 32768},\n",
       "  'stft': {'filter_length': 1024, 'hop_length': 160, 'win_length': 1024},\n",
       "  'mel': {'n_mel_channels': 64,\n",
       "   'mel_fmin': 0,\n",
       "   'mel_fmax': 8000,\n",
       "   'freqm': 0,\n",
       "   'timem': 0,\n",
       "   'blur': False,\n",
       "   'mean': -4.63,\n",
       "   'std': 2.74,\n",
       "   'target_length': 1024}},\n",
       " 'model': {'device': 'cuda',\n",
       "  'target': 'audioldm.pipline.LatentDiffusion',\n",
       "  'params': {'base_learning_rate': 5e-06,\n",
       "   'linear_start': 0.0015,\n",
       "   'linear_end': 0.0195,\n",
       "   'num_timesteps_cond': 1,\n",
       "   'log_every_t': 200,\n",
       "   'timesteps': 1000,\n",
       "   'first_stage_key': 'fbank',\n",
       "   'cond_stage_key': 'waveform',\n",
       "   'latent_t_size': 256,\n",
       "   'latent_f_size': 16,\n",
       "   'channels': 8,\n",
       "   'cond_stage_trainable': True,\n",
       "   'conditioning_key': 'film',\n",
       "   'monitor': 'val/loss_simple_ema',\n",
       "   'scale_by_std': True,\n",
       "   'unet_config': {'target': 'audioldm.latent_diffusion.openaimodel.UNetModel',\n",
       "    'params': {'image_size': 64,\n",
       "     'extra_film_condition_dim': 512,\n",
       "     'extra_film_use_concat': True,\n",
       "     'in_channels': 8,\n",
       "     'out_channels': 8,\n",
       "     'model_channels': 128,\n",
       "     'attention_resolutions': [8, 4, 2],\n",
       "     'num_res_blocks': 2,\n",
       "     'channel_mult': [1, 2, 3, 5],\n",
       "     'num_head_channels': 32,\n",
       "     'use_spatial_transformer': True}},\n",
       "   'first_stage_config': {'base_learning_rate': 4.5e-05,\n",
       "    'target': 'audioldm.variational_autoencoder.autoencoder.AutoencoderKL',\n",
       "    'params': {'monitor': 'val/rec_loss',\n",
       "     'image_key': 'fbank',\n",
       "     'subband': 1,\n",
       "     'embed_dim': 8,\n",
       "     'time_shuffle': 1,\n",
       "     'ddconfig': {'double_z': True,\n",
       "      'z_channels': 8,\n",
       "      'resolution': 256,\n",
       "      'downsample_time': False,\n",
       "      'in_channels': 1,\n",
       "      'out_ch': 1,\n",
       "      'ch': 128,\n",
       "      'ch_mult': [1, 2, 4],\n",
       "      'num_res_blocks': 2,\n",
       "      'attn_resolutions': [],\n",
       "      'dropout': 0.0}}},\n",
       "   'cond_stage_config': {'target': 'audioldm.clap.encoders.CLAPAudioEmbeddingClassifierFreev2',\n",
       "    'params': {'key': 'waveform',\n",
       "     'sampling_rate': 16000,\n",
       "     'embed_mode': 'audio',\n",
       "     'unconditional_prob': 0.1}}}}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from audioldm.utils import default_audioldm_config\n",
    "config = default_audioldm_config()\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v2a-mapper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
