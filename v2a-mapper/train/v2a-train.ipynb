{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e0e065",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm, trange\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482cff27",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"/mnt/new_volume2/vgg_sound_emb\"\n",
    "partition = \"train\"\n",
    "data_dir = f\"{root}/{partition}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7428af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "root = \"/mnt/new_volume2/vgg_sound_emb\"\n",
    "partition = \"train\"\n",
    "data_dir = f\"{root}/{partition}\"\n",
    "\n",
    "class LargeVideoDataset(Dataset):\n",
    "    def __init__(self, data_dir, subset_ratio = 0.2, transform=None):\n",
    "        \"\"\"\n",
    "        root_dir: 保存所有 .pth 文件的目录，每个文件对应一个 sample。\n",
    "        transform: 如果需要对数据做预处理，可在这里传入。\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # 仅收集当前目录下所有的 pth 文件列表\n",
    "        file_list = []\n",
    "\n",
    "        for root, dirs, files in os.walk(data_dir):\n",
    "            for file in files:\n",
    "                if file.endswith(\".pth\"):\n",
    "                    file_list.append(os.path.join(root, file))\n",
    "\n",
    "        # 仅使用前 20% 的数据\n",
    "        num_samples = int(len(file_list) * subset_ratio)\n",
    "\n",
    "        self.file_paths = sorted(file_list)[:num_samples]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 在这里按需读取，而不是一次性加载全部\n",
    "        pth_path = self.file_paths[idx]\n",
    "        sample_data = torch.load(pth_path)  \n",
    "        clip_feat = sample_data['clip_features']  # (64, 512)\n",
    "        clap_feat = sample_data['clap_features']  # (1, 512)\n",
    "\n",
    "        if self.transform:\n",
    "            clip_feat, clap_feat = self.transform((clip_feat, clap_feat))\n",
    "\n",
    "        return clip_feat, clap_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bf25d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_sound = LargeVideoDataset(data_dir, subset_ratio = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2773fe35",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb52cb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ratio = 0.1\n",
    "test_ratio = 0.1\n",
    "\n",
    "total_len = len(vgg_sound)\n",
    "val_len = int(total_len * val_ratio)\n",
    "test_len = int(total_len * test_ratio)\n",
    "train_len = total_len - val_len - test_len\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    vgg_sound, [train_len, val_len, test_len], generator=torch.Generator().manual_seed(42), \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72a6a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_workers = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf89ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a42640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clip Feature:  torch.Size([64, 64, 512])\n",
      "CLAP Feature:  torch.Size([64, 1, 512])\n"
     ]
    }
   ],
   "source": [
    "# get next batch from train_loader\n",
    "next_batch = next(iter(train_loader))\n",
    "clip_feat, clap_feat = next_batch\n",
    "print(\"Clip Feature: \", clip_feat.shape)  # (64, 64, 512)\n",
    "print(\"CLAP Feature: \", clap_feat.shape)  # (64, 1, 512)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52ac4b0",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562389b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class V2AMapperMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    将(64,512)的clip特征先池化到(1,512),\n",
    "    再映射到(1,512).\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=512, hidden_dim=1024, output_dim=512):\n",
    "        super().__init__()\n",
    "        # 可以先做一个简单的线性层, 或者堆叠多层\n",
    "        self.pooling = nn.AdaptiveAvgPool2d((1, input_dim))  \n",
    "        # pooling后, shape变成 (1, input_dim)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, 64, 512)\n",
    "        # 先把 shape (B,64,512) pooling 到 (B,1,512)\n",
    "        # 这里可以用简单的mean替代，也可以用AdaptiveAvgPool2d\n",
    "        pooled = x.mean(dim=1)  # (B,512)\n",
    "\n",
    "        # 送入多层感知机映射到(512)\n",
    "        out = self.mlp(pooled)  # (B,512)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71df96b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, scaler):\n",
    "\n",
    "    model.train()\n",
    "    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train')\n",
    "\n",
    "    total_loss = 0\n",
    "    for i, (clip_feat, clap_feat) in enumerate(train_loader):\n",
    "        clip_feat = clip_feat.to(device)\n",
    "        clap_feat = clap_feat.to(device)\n",
    "\n",
    "        # 前向传播\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(clip_feat)  \n",
    "            loss = criterion(outputs, clap_feat.squeeze(1)) \n",
    "\n",
    "        # 反向传播\n",
    "        # loss.backward()\n",
    "        # optimizer.step()\n",
    "\n",
    "        scaler.scale(loss).backward() # This is a replacement for loss.backward()\n",
    "        scaler.step(optimizer) # This is a replacement for optimizer.step()\n",
    "        scaler.update() # This is something added just for FP16\n",
    "\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        batch_bar.set_postfix(\n",
    "            loss=\"{:.04f}\".format(float(total_loss / (i + 1))),\n",
    "            lr=\"{:.06f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
    "\n",
    "        batch_bar.update() # Update tqdm bar\n",
    "\n",
    "        del clip_feat, clap_feat, outputs, loss\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbfc90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def validate_model(model, val_loader, criterion, optimizer):\n",
    "\n",
    "    model.eval()\n",
    "    batch_bar = tqdm(total=len(val_loader), dynamic_ncols=True, position=0, leave=False, desc='Val')\n",
    "\n",
    "    total_loss = 0\n",
    "    vdist = 0\n",
    "    for i, (clip_feat, clap_feat) in enumerate(val_loader):\n",
    "        clip_feat = clip_feat.to(device)\n",
    "        clap_feat = clap_feat.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(clip_feat)  \n",
    "            loss = criterion(outputs, clap_feat.squeeze(1)) \n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        batch_bar.set_postfix(\n",
    "            loss=\"{:.04f}\".format(float(total_loss / (i + 1))),\n",
    "            lr=\"{:.06f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
    "\n",
    "        batch_bar.update() # Update tqdm bar\n",
    "        del clip_feat, clap_feat, outputs, loss\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    batch_bar.close()\n",
    "    return total_loss / len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959b928a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, criterion, optimizer,scaler, ckpt_dir, num_epochs=10):\n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        train_loss = train_model(model, train_loader, criterion, optimizer, scaler)\n",
    "        val_loss = validate_model(model, val_loader, criterion, optimizer)\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Save the model if validation loss has decreased\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), ckpt_dir + \"best_model.pth\")\n",
    "            print(\"Model saved!\")\n",
    "        else:\n",
    "            print(\"Validation loss did not improve, model not saved.\")\n",
    "    print(\"Training complete!\")\n",
    "    print(f\"Best validation loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d90790",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CosineLoss(nn.Module):\n",
    "    def __init__(self, margin=0.8):  # 建议 margin 设得稍高一点\n",
    "        super(CosineLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        # output: (B, 512)\n",
    "        # target: (B, 1, 512) or (B, 512)\n",
    "        if target.ndim == 3:\n",
    "            target = target.squeeze(1)\n",
    "        cos_sim = F.cosine_similarity(output, target, dim=1)  # (B,)\n",
    "        loss = torch.mean(torch.clamp(self.margin - cos_sim, min=0))\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59fc4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2021/1702422791.py:8: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   0%|          | 0/229 [00:00<?, ?it/s]/tmp/ipykernel_2021/3516177890.py:13: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5001, Val Loss: 0.4590\n",
      "Model saved!\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4443, Val Loss: 0.4270\n",
      "Model saved!\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4241, Val Loss: 0.4127\n",
      "Model saved!\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4110, Val Loss: 0.4058\n",
      "Model saved!\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3995, Val Loss: 0.3974\n",
      "Model saved!\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3902, Val Loss: 0.3908\n",
      "Model saved!\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3822, Val Loss: 0.3854\n",
      "Model saved!\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3748, Val Loss: 0.3849\n",
      "Model saved!\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3683, Val Loss: 0.3840\n",
      "Model saved!\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3620, Val Loss: 0.3809\n",
      "Model saved!\n",
      "Training complete!\n",
      "Best validation loss: 0.3809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "lr = 0.001\n",
    "\n",
    "model = V2AMapperMLP(input_dim=512, hidden_dim=1024, output_dim=512).to(device)\n",
    "# criterion = nn.MSELoss()\n",
    "criterion = CosineLoss(margin= 1).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "# scheduler = torch.optim.CosineAnnealingLR(optimizer, T_max=10, eta_min=0.00001)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "ckpt_dir = \"checkpoints\"\n",
    "\n",
    "train(model, train_loader, val_loader, criterion, optimizer,scaler, ckpt_dir = \"ckpts/\", num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109c2932",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v2a-mapper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
