{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/v2a-mapper/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/ubuntu/miniconda3/envs/v2a-mapper/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "/home/ubuntu/miniconda3/envs/v2a-mapper/lib/python3.11/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import torch\n",
    "import laion_clap\n",
    "from extraction.vgg_sound import *\n",
    "from audioldm import image_to_audio, build_model, clap_to_audio\n",
    "import torch\n",
    "import torchaudio\n",
    "from transformers import AutoProcessor, AutoModel\n",
    "import soundfile as sf \n",
    "import clip\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "# model.eval()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 338M/338M [00:01<00:00, 177MiB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CLIP(\n",
       "  (visual): VisionTransformer(\n",
       "    (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): Sequential(\n",
       "        (0): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (resblocks): Sequential(\n",
       "      (0): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (6): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (7): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (8): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (9): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (10): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (11): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (token_embedding): Embedding(49408, 512)\n",
       "  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import clip\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "clip_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:199176 videos found in /mnt/new_volume/vgg_sound/scratch/shared/beegfs/hchen/train_data/VGGSound_final/video\n",
      "INFO:root:15446 videos found in /mnt/new_volume/vgg_sound/test.csv\n",
      "INFO:root:0 videos missing in /mnt/new_volume/vgg_sound/scratch/shared/beegfs/hchen/train_data/VGGSound_final/video\n"
     ]
    }
   ],
   "source": [
    "vgg_dataset = VGGSound(root=\"/mnt/new_volume/vgg_sound/scratch/shared/beegfs/hchen/train_data/VGGSound_final/video\",\n",
    "                           csv_path=\"/mnt/new_volume/vgg_sound/test.csv\",\n",
    "                           sample_rate =  16000)\n",
    "\n",
    "\n",
    "# data = vgg_dataset[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_rate: 44100\n",
      "self sample:  16000\n"
     ]
    }
   ],
   "source": [
    "img = vgg_dataset[0]['clip_video']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = img.to(device)\n",
    "# with torch.no_grad():\n",
    "#     image_embedding = clip_model.encode_image(img) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    image_embedding = clip_model.encode_image(img) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 224, 224])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    image_embedding_1 = clip_model.encode_image(img[0].unsqueeze(0).to(device)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.4961e-01,  1.2732e-01, -1.5881e-01,  9.8572e-02,  1.6345e-01,\n",
       "         -1.9629e-01,  6.2646e-01,  1.9263e-01, -1.3049e-01,  5.4047e-02,\n",
       "         -4.6906e-02, -1.9800e-01,  1.0040e-01, -4.1724e-01,  3.0151e-01,\n",
       "         -3.0176e-01, -1.8291e+00, -2.2766e-02,  2.1692e-01, -4.6851e-01,\n",
       "          2.2827e-01,  1.1505e-01,  1.2891e-01,  2.0276e-01, -6.5079e-03,\n",
       "         -1.7041e-01, -4.4861e-02, -2.9272e-01,  1.7920e-01,  1.2939e-01,\n",
       "         -4.6448e-02,  5.4474e-02, -5.4102e-01, -6.2164e-02, -1.0303e-01,\n",
       "          1.7212e-01, -3.1421e-01,  3.7231e-02,  1.4575e-01, -7.4902e-01,\n",
       "         -6.3916e-01, -1.5308e-01, -2.6807e-01, -4.7534e-01, -1.4783e-01,\n",
       "          8.4961e-01,  5.4053e-01, -2.0544e-01, -3.2642e-01,  3.0273e-01,\n",
       "          5.1514e-01, -3.4399e-01,  1.3745e-01,  2.6758e-01,  2.0874e-01,\n",
       "          6.0596e-01, -1.9226e-01,  2.3254e-02, -7.5012e-02,  8.9741e-04,\n",
       "          1.2646e-01, -7.2693e-02,  4.0381e-01,  1.1780e-01,  1.5564e-01,\n",
       "          3.9697e-01, -5.2783e-01,  1.0291e-01, -1.5051e-01, -1.0693e-01,\n",
       "         -8.3374e-02,  9.4116e-02, -1.6431e-01,  1.3489e-01,  3.8623e-01,\n",
       "          1.6418e-02,  4.0405e-01, -4.5319e-03, -9.1187e-02, -4.5337e-01,\n",
       "          7.7698e-02, -3.5620e-01,  5.1514e-02, -8.8562e-02,  1.8274e-01,\n",
       "          1.4856e-01,  1.0527e+00,  2.5803e-02,  4.8389e-01, -3.9551e-01,\n",
       "          1.8604e-01,  2.0789e-01, -7.7422e+00, -8.1543e-01,  9.7229e-02,\n",
       "          5.8496e-01, -1.1224e-01, -2.6050e-01, -5.0195e-01, -1.5654e+00,\n",
       "          1.3916e-01,  6.0486e-02,  3.9111e-01,  5.9998e-02, -4.3457e-02,\n",
       "          5.9967e-02, -1.2900e+00, -3.0029e-01, -3.9111e-01, -3.3301e-01,\n",
       "          4.0747e-01,  5.0476e-02,  1.9275e-01, -2.3547e-01,  4.9042e-02,\n",
       "         -4.6631e-01, -3.7720e-01,  3.2446e-01,  1.4893e-01, -4.6191e-01,\n",
       "          3.1421e-01, -4.7388e-01,  1.3049e-01,  3.4375e-01,  4.3530e-01,\n",
       "         -1.2927e-01,  1.6800e-02,  7.7148e-02,  4.3213e-01, -2.4976e-01,\n",
       "          9.4971e-02, -6.4746e-01, -4.4238e-01,  9.3555e-01,  5.8008e-01,\n",
       "          2.4683e-01,  3.3740e-01, -2.5220e-01, -8.8086e-01, -1.9470e-02,\n",
       "          2.5342e-01,  2.6489e-01,  3.4106e-01,  2.5299e-02, -3.1592e-01,\n",
       "         -1.7609e-02, -2.5098e-01,  3.4595e-01, -1.8628e-01, -4.3994e-01,\n",
       "          3.2471e-01, -3.8843e-01,  6.1279e-01,  1.0077e-01,  5.9717e-01,\n",
       "         -3.1689e-01,  2.6318e-01, -4.1284e-01, -1.5540e-01, -6.8604e-02,\n",
       "          5.6592e-01,  1.9434e-01, -1.3513e-01,  2.9980e-01, -3.3643e-01,\n",
       "          3.5004e-02,  5.5713e-01,  1.3159e-01, -1.8433e-01,  4.9377e-02,\n",
       "          9.2346e-02, -5.6592e-01, -2.7222e-01, -3.2251e-01, -3.0151e-01,\n",
       "          9.1476e-03,  9.7021e-01,  2.9736e-01,  1.0938e-01,  3.2837e-01,\n",
       "          2.2640e-03, -4.8999e-01,  2.4841e-01, -5.1239e-02, -1.7065e-01,\n",
       "          2.3755e-01, -3.9624e-01, -1.3916e-01,  3.7573e-01,  1.8396e-01,\n",
       "          2.0984e-01,  3.6719e-01, -7.1716e-02,  2.3669e-01, -1.3013e-01,\n",
       "         -1.2646e-01, -2.0764e-01, -1.3953e-01, -1.0459e+00,  6.7139e-02,\n",
       "          5.5762e-01,  9.8206e-02, -4.1089e-01, -1.4087e-01,  2.2375e-01,\n",
       "          1.1444e-01, -5.9131e-01, -2.2473e-01, -2.7588e-01, -2.5269e-01,\n",
       "         -1.4502e-01,  1.1777e+00,  4.5105e-02, -2.6413e-02,  2.7295e-01,\n",
       "          5.8197e-02, -8.0933e-02,  5.2002e-01, -3.1543e-01,  7.1411e-02,\n",
       "         -2.8784e-01,  4.5380e-02,  1.7212e-01,  1.7566e-01,  1.1298e-01,\n",
       "         -2.1484e-02, -2.1576e-02, -9.3079e-02, -3.2642e-01,  1.2073e-01,\n",
       "          2.1881e-02, -9.8511e-02,  5.8105e-01,  4.9829e-01,  3.4119e-02,\n",
       "          9.4238e-01, -7.9163e-02,  2.8394e-01,  1.2646e-01, -1.8845e-02,\n",
       "         -2.4316e-01, -2.9688e-01,  3.1006e-01,  2.3022e-01, -2.7734e-01,\n",
       "          2.5806e-01,  1.4908e-02,  4.9805e-01,  4.9896e-02, -1.1454e-03,\n",
       "          4.8615e-02, -4.2206e-02, -5.9204e-03,  2.0618e-01, -2.9443e-01,\n",
       "          1.6492e-01, -5.7251e-02,  5.1807e-01, -1.4814e+00,  6.9482e-01,\n",
       "          4.0015e-01, -3.5431e-02,  3.8135e-01, -1.1689e+00, -1.3440e-01,\n",
       "          1.0901e-01,  3.5815e-01, -4.8804e-01, -5.9479e-02,  7.0984e-02,\n",
       "          4.5557e-01,  3.3081e-01, -3.9087e-01, -8.6670e-02,  2.7417e-01,\n",
       "         -1.2720e-01, -2.6270e-01,  3.0688e-01,  4.6814e-02, -1.8774e-01,\n",
       "         -2.5732e-01,  1.7847e-01,  2.1619e-01,  4.0016e-03, -1.8494e-01,\n",
       "         -8.5571e-02,  1.1035e+00,  1.4417e-01,  9.1309e-02,  3.0106e-02,\n",
       "          7.0572e-03, -9.8267e-02,  2.7979e-01, -1.1877e-01, -2.1521e-01,\n",
       "          5.4346e-01, -8.1348e-01,  1.9348e-01,  2.2925e-01, -1.3501e-01,\n",
       "          7.8918e-02, -8.5632e-02, -8.2227e-01, -3.0151e-01, -1.3245e-01,\n",
       "         -4.5239e-01, -2.9883e-01, -1.7114e-01,  3.7329e-01,  1.3269e-01,\n",
       "         -1.4490e-01,  3.2056e-01,  9.3311e-01,  1.3538e-01,  9.4986e-03,\n",
       "          1.8774e-01,  4.4043e-01,  5.2344e-01,  3.1104e-01, -1.9824e-01,\n",
       "          4.7119e-01,  8.7695e-01, -2.4670e-01, -1.0211e-01, -5.4736e-01,\n",
       "         -2.1704e-01,  5.2002e-01, -5.6458e-03, -2.3682e-02,  2.0728e-01,\n",
       "          5.0018e-02,  4.3945e-01,  3.1891e-02,  5.9131e-01, -5.1025e-01,\n",
       "          1.3908e-02,  1.8921e-01, -3.3179e-01, -3.5736e-02,  4.9463e-01,\n",
       "          4.5312e-01, -1.3757e-01,  1.8323e-01,  1.6943e-01, -2.7191e-02,\n",
       "         -4.8828e-02, -2.6270e-01, -2.3499e-01,  7.6782e-02, -7.5989e-02,\n",
       "         -4.1772e-01, -2.0312e-01, -2.9419e-01, -1.5881e-01,  3.0249e-01,\n",
       "          1.4648e-01,  1.7212e-02,  4.5264e-01,  8.3496e-02,  1.3806e-01,\n",
       "         -6.9287e-01, -3.9978e-02, -1.9739e-01,  8.5754e-02, -7.5439e-02,\n",
       "         -2.3621e-02, -2.6025e-01, -9.7559e-01,  2.9492e-01,  1.4160e-01,\n",
       "          3.3295e-02,  6.6406e-02,  6.9275e-02, -3.0884e-02,  4.2285e-01,\n",
       "          5.2124e-02,  1.8640e-01, -1.3135e-01, -3.7012e-01, -1.2009e-02,\n",
       "          3.1543e-01, -5.0977e-01, -2.5610e-01, -3.1812e-01, -2.3279e-01,\n",
       "         -5.1270e-01,  2.1500e-02,  9.3457e-01, -3.2153e-01,  1.7754e+00,\n",
       "          2.6398e-02, -3.1982e-01, -2.0300e-01, -5.6305e-02, -3.0005e-01,\n",
       "         -3.1934e-01,  2.0300e-01,  2.0435e-01, -5.4150e-01,  7.6965e-02,\n",
       "          1.6223e-01,  3.4241e-02,  3.7427e-01,  1.0059e-01,  2.7856e-01,\n",
       "          1.1597e-01,  5.1849e-02, -1.3452e-01,  1.0980e-01, -3.7329e-01,\n",
       "          2.3157e-01, -2.9736e-01, -1.1389e-01,  5.2539e-01,  5.0018e-02,\n",
       "         -3.2275e-01,  1.5344e-01,  3.0121e-02, -2.8247e-01, -4.3488e-03,\n",
       "          6.2164e-02,  2.5464e-01,  2.4072e-01, -2.8412e-02, -4.2871e-01,\n",
       "         -1.4978e-01, -5.6201e-01,  1.8884e-01,  1.8103e-01,  2.2913e-01,\n",
       "         -2.8223e-01,  2.5244e-01, -4.5044e-01, -5.0000e-01, -9.1919e-02,\n",
       "         -1.4551e-01,  2.8107e-02,  2.0044e-01, -1.4062e-01, -4.4922e-02,\n",
       "         -8.1970e-02,  6.7444e-02, -2.5757e-01,  3.3228e-01,  2.0862e-01,\n",
       "         -3.3667e-01,  5.6335e-02,  2.9028e-01,  9.0149e-02,  2.4084e-01,\n",
       "          2.4292e-01, -1.0791e-01,  2.8296e-01, -4.8413e-01,  1.3623e-01,\n",
       "          5.1074e-01,  4.6948e-01, -7.1472e-02,  6.0730e-02,  3.5278e-01,\n",
       "          1.5381e-01, -1.3635e-01, -8.2016e-03, -4.9536e-01,  5.2100e-01,\n",
       "         -1.8799e-01,  3.9990e-01, -5.8624e-02,  8.9417e-02, -3.0624e-02,\n",
       "         -1.6699e-01, -3.4523e-03, -2.5692e-03, -3.2324e-01, -3.2715e-01,\n",
       "          1.4783e-01, -2.3059e-01,  1.1487e-01,  1.1810e-01, -2.1960e-01,\n",
       "         -4.4159e-02, -7.7576e-02, -9.2651e-02,  2.0312e-01,  2.1815e-04,\n",
       "         -2.6733e-01, -4.6362e-01,  2.1194e-02, -1.2830e-01,  2.7808e-01,\n",
       "          8.1738e-01, -1.8469e-01, -6.1584e-02, -3.1543e-01,  1.8845e-02,\n",
       "         -4.8950e-01, -8.6853e-02,  2.2034e-01,  1.4355e-01, -1.2244e-01,\n",
       "          1.5723e-01, -4.3549e-02, -5.3467e-01,  1.4124e-01,  1.0669e-01,\n",
       "         -1.1023e-01, -2.3486e-01,  2.7075e-01, -7.3303e-02,  4.6338e-01,\n",
       "         -1.0870e-01,  1.2372e-01]], device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_embedding_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3.4985e-01,  1.2634e-01, -1.5808e-01,  9.9182e-02,  1.6418e-01,\n",
       "        -1.9592e-01,  6.2646e-01,  1.9360e-01, -1.2854e-01,  5.4626e-02,\n",
       "        -4.6326e-02, -1.9714e-01,  1.0095e-01, -4.1748e-01,  3.0151e-01,\n",
       "        -3.0176e-01, -1.8291e+00, -2.3361e-02,  2.1667e-01, -4.6851e-01,\n",
       "         2.2705e-01,  1.1414e-01,  1.2927e-01,  2.0386e-01, -7.0190e-03,\n",
       "        -1.7065e-01, -4.4312e-02, -2.9272e-01,  1.7822e-01,  1.2939e-01,\n",
       "        -4.6265e-02,  5.4413e-02, -5.4053e-01, -6.1920e-02, -1.0248e-01,\n",
       "         1.7175e-01, -3.1421e-01,  3.7659e-02,  1.4526e-01, -7.5000e-01,\n",
       "        -6.3965e-01, -1.5271e-01, -2.6855e-01, -4.7559e-01, -1.4819e-01,\n",
       "         8.5303e-01,  5.4053e-01, -2.0520e-01, -3.2666e-01,  3.0249e-01,\n",
       "         5.1465e-01, -3.4351e-01,  1.3733e-01,  2.6660e-01,  2.0898e-01,\n",
       "         6.0596e-01, -1.9250e-01,  2.2675e-02, -7.4158e-02,  1.6327e-03,\n",
       "         1.2732e-01, -7.2693e-02,  4.0308e-01,  1.1841e-01,  1.5576e-01,\n",
       "         3.9673e-01, -5.2686e-01,  1.0413e-01, -1.5051e-01, -1.0657e-01,\n",
       "        -8.3374e-02,  9.4421e-02, -1.6345e-01,  1.3440e-01,  3.8550e-01,\n",
       "         1.6174e-02,  4.0332e-01, -4.5395e-03, -9.1675e-02, -4.5410e-01,\n",
       "         7.7454e-02, -3.5645e-01,  5.0934e-02, -8.7769e-02,  1.8176e-01,\n",
       "         1.4893e-01,  1.0537e+00,  2.5192e-02,  4.8193e-01, -3.9502e-01,\n",
       "         1.8628e-01,  2.0752e-01, -7.7422e+00, -8.1396e-01,  9.7595e-02,\n",
       "         5.8496e-01, -1.1304e-01, -2.5977e-01, -5.0098e-01, -1.5645e+00,\n",
       "         1.3965e-01,  5.9662e-02,  3.9136e-01,  5.9967e-02, -4.3640e-02,\n",
       "         6.0699e-02, -1.2910e+00, -2.9932e-01, -3.9160e-01, -3.3252e-01,\n",
       "         4.0723e-01,  5.0507e-02,  1.9238e-01, -2.3499e-01,  4.9438e-02,\n",
       "        -4.6606e-01, -3.7720e-01,  3.2495e-01,  1.4905e-01, -4.6191e-01,\n",
       "         3.1372e-01, -4.7339e-01,  1.2964e-01,  3.4399e-01,  4.3555e-01,\n",
       "        -1.2927e-01,  1.5900e-02,  7.7026e-02,  4.3115e-01, -2.5024e-01,\n",
       "         9.5398e-02, -6.4844e-01, -4.4189e-01,  9.3604e-01,  5.8008e-01,\n",
       "         2.4707e-01,  3.3789e-01, -2.5293e-01, -8.8184e-01, -1.9394e-02,\n",
       "         2.5293e-01,  2.6465e-01,  3.4229e-01,  2.4506e-02, -3.1665e-01,\n",
       "        -1.7609e-02, -2.5098e-01,  3.4619e-01, -1.8567e-01, -4.3970e-01,\n",
       "         3.2422e-01, -3.8794e-01,  6.1279e-01,  1.0028e-01,  5.9766e-01,\n",
       "        -3.1787e-01,  2.6343e-01, -4.1284e-01, -1.5527e-01, -6.8176e-02,\n",
       "         5.6543e-01,  1.9421e-01, -1.3464e-01,  2.9932e-01, -3.3618e-01,\n",
       "         3.5706e-02,  5.5664e-01,  1.3171e-01, -1.8530e-01,  4.9286e-02,\n",
       "         9.2712e-02, -5.6592e-01, -2.7148e-01, -3.2153e-01, -3.0200e-01,\n",
       "         1.0345e-02,  9.7070e-01,  2.9736e-01,  1.0852e-01,  3.2812e-01,\n",
       "         2.7695e-03, -4.8950e-01,  2.4890e-01, -5.1636e-02, -1.7029e-01,\n",
       "         2.3718e-01, -3.9648e-01, -1.4001e-01,  3.7524e-01,  1.8359e-01,\n",
       "         2.0959e-01,  3.6743e-01, -7.1655e-02,  2.3682e-01, -1.3049e-01,\n",
       "        -1.2634e-01, -2.0923e-01, -1.4026e-01, -1.0459e+00,  6.8115e-02,\n",
       "         5.5713e-01,  9.8206e-02, -4.1089e-01, -1.4038e-01,  2.2266e-01,\n",
       "         1.1456e-01, -5.9131e-01, -2.2400e-01, -2.7515e-01, -2.5122e-01,\n",
       "        -1.4587e-01,  1.1787e+00,  4.4861e-02, -2.6169e-02,  2.7319e-01,\n",
       "         5.8228e-02, -8.0444e-02,  5.1904e-01, -3.1396e-01,  7.0435e-02,\n",
       "        -2.8638e-01,  4.6112e-02,  1.7151e-01,  1.7615e-01,  1.1304e-01,\n",
       "        -2.1332e-02, -2.1744e-02, -9.3689e-02, -3.2568e-01,  1.2091e-01,\n",
       "         2.3071e-02, -9.9365e-02,  5.8105e-01,  4.9780e-01,  3.4637e-02,\n",
       "         9.4189e-01, -7.8979e-02,  2.8394e-01,  1.2659e-01, -1.9714e-02,\n",
       "        -2.4304e-01, -2.9712e-01,  3.1030e-01,  2.3108e-01, -2.7783e-01,\n",
       "         2.5708e-01,  1.4511e-02,  4.9805e-01,  4.9927e-02, -2.9445e-04,\n",
       "         4.8035e-02, -4.1901e-02, -6.2447e-03,  2.0605e-01, -2.9395e-01,\n",
       "         1.6443e-01, -5.6549e-02,  5.1855e-01, -1.4805e+00,  6.9482e-01,\n",
       "         3.9990e-01, -3.6102e-02,  3.8159e-01, -1.1699e+00, -1.3464e-01,\n",
       "         1.0834e-01,  3.5840e-01, -4.8828e-01, -5.9784e-02,  7.2083e-02,\n",
       "         4.5581e-01,  3.3032e-01, -3.9136e-01, -8.6365e-02,  2.7344e-01,\n",
       "        -1.2744e-01, -2.6270e-01,  3.0566e-01,  4.6600e-02, -1.8811e-01,\n",
       "        -2.5708e-01,  1.7908e-01,  2.1643e-01,  5.4245e-03, -1.8506e-01,\n",
       "        -8.4900e-02,  1.1045e+00,  1.4380e-01,  9.1003e-02,  2.9999e-02,\n",
       "         6.5422e-03, -9.8816e-02,  2.7954e-01, -1.1841e-01, -2.1497e-01,\n",
       "         5.4395e-01, -8.1348e-01,  1.9373e-01,  2.2876e-01, -1.3513e-01,\n",
       "         7.9468e-02, -8.5938e-02, -8.2227e-01, -3.0054e-01, -1.3257e-01,\n",
       "        -4.5410e-01, -2.9858e-01, -1.7090e-01,  3.7329e-01,  1.3293e-01,\n",
       "        -1.4551e-01,  3.2129e-01,  9.3359e-01,  1.3489e-01,  9.3536e-03,\n",
       "         1.8823e-01,  4.4092e-01,  5.2246e-01,  3.1055e-01, -1.9800e-01,\n",
       "         4.7217e-01,  8.7695e-01, -2.4646e-01, -1.0217e-01, -5.4736e-01,\n",
       "        -2.1814e-01,  5.2051e-01, -5.5618e-03, -2.3499e-02,  2.0776e-01,\n",
       "         5.0446e-02,  4.3921e-01,  3.2257e-02,  5.9180e-01, -5.1074e-01,\n",
       "         1.3878e-02,  1.8872e-01, -3.3179e-01, -3.5736e-02,  4.9487e-01,\n",
       "         4.5337e-01, -1.3721e-01,  1.8372e-01,  1.7029e-01, -2.7435e-02,\n",
       "        -4.7699e-02, -2.6172e-01, -2.3474e-01,  7.6050e-02, -7.5256e-02,\n",
       "        -4.1553e-01, -2.0251e-01, -2.9419e-01, -1.5857e-01,  3.0322e-01,\n",
       "         1.4575e-01,  1.7242e-02,  4.5288e-01,  8.2764e-02,  1.3892e-01,\n",
       "        -6.9385e-01, -3.9124e-02, -1.9812e-01,  8.5327e-02, -7.4890e-02,\n",
       "        -2.2980e-02, -2.6074e-01, -9.7656e-01,  2.9492e-01,  1.4185e-01,\n",
       "         3.3478e-02,  6.5796e-02,  6.9275e-02, -3.0426e-02,  4.2334e-01,\n",
       "         5.2765e-02,  1.8628e-01, -1.3110e-01, -3.7061e-01, -1.1459e-02,\n",
       "         3.1519e-01, -5.0879e-01, -2.5659e-01, -3.1836e-01, -2.3267e-01,\n",
       "        -5.1172e-01,  2.1317e-02,  9.3408e-01, -3.2178e-01,  1.7744e+00,\n",
       "         2.5101e-02, -3.2007e-01, -2.0300e-01, -5.6244e-02, -2.9980e-01,\n",
       "        -3.1982e-01,  2.0276e-01,  2.0459e-01, -5.4102e-01,  7.7271e-02,\n",
       "         1.6223e-01,  3.4607e-02,  3.7476e-01,  1.0052e-01,  2.7881e-01,\n",
       "         1.1578e-01,  5.1544e-02, -1.3538e-01,  1.0999e-01, -3.7451e-01,\n",
       "         2.3230e-01, -2.9688e-01, -1.1456e-01,  5.2539e-01,  4.9561e-02,\n",
       "        -3.2202e-01,  1.5393e-01,  3.0731e-02, -2.8223e-01, -4.8141e-03,\n",
       "         6.1981e-02,  2.5464e-01,  2.3962e-01, -2.8503e-02, -4.2847e-01,\n",
       "        -1.4941e-01, -5.6152e-01,  1.8896e-01,  1.8005e-01,  2.2998e-01,\n",
       "        -2.8296e-01,  2.5269e-01, -4.5166e-01, -4.9951e-01, -9.2346e-02,\n",
       "        -1.4429e-01,  2.7649e-02,  2.0129e-01, -1.4087e-01, -4.4525e-02,\n",
       "        -8.2520e-02,  6.7444e-02, -2.5757e-01,  3.3252e-01,  2.0923e-01,\n",
       "        -3.3716e-01,  5.6793e-02,  2.9028e-01,  8.9478e-02,  2.4048e-01,\n",
       "         2.4377e-01, -1.0773e-01,  2.8271e-01, -4.8291e-01,  1.3611e-01,\n",
       "         5.1172e-01,  4.6973e-01, -7.1045e-02,  6.1157e-02,  3.5254e-01,\n",
       "         1.5393e-01, -1.3660e-01, -7.2975e-03, -4.9536e-01,  5.2051e-01,\n",
       "        -1.8726e-01,  4.0015e-01, -5.8319e-02,  8.9600e-02, -2.9938e-02,\n",
       "        -1.6736e-01, -2.8648e-03, -2.3518e-03, -3.2324e-01, -3.2690e-01,\n",
       "         1.4758e-01, -2.3059e-01,  1.1462e-01,  1.1847e-01, -2.2058e-01,\n",
       "        -4.4281e-02, -7.8552e-02, -9.2285e-02,  2.0361e-01,  4.3511e-06,\n",
       "        -2.6709e-01, -4.6313e-01,  2.1576e-02, -1.2988e-01,  2.7759e-01,\n",
       "         8.1836e-01, -1.8481e-01, -6.2134e-02, -3.1543e-01,  1.9592e-02,\n",
       "        -4.8901e-01, -8.7280e-02,  2.2083e-01,  1.4392e-01, -1.2225e-01,\n",
       "         1.5637e-01, -4.3365e-02, -5.3418e-01,  1.4038e-01,  1.0657e-01,\n",
       "        -1.0950e-01, -2.3389e-01,  2.7051e-01, -7.2815e-02,  4.6362e-01,\n",
       "        -1.0870e-01,  1.2384e-01], device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_embedding[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/vocab.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/vocab.json HTTP/1.1\" 200 0\n",
      "INFO:root:Loading HTSAT-tiny model config.\n",
      "/home/ubuntu/miniconda3/envs/v2a-mapper/lib/python3.11/site-packages/torchlibrosa/stft.py:193: FutureWarning: Pass size=1024 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  fft_window = librosa.util.pad_center(fft_window, n_fft)\n",
      "/home/ubuntu/miniconda3/envs/v2a-mapper/lib/python3.11/site-packages/torch/functional.py:539: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:3637.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO:root:Loading pretrained HTSAT-tiny-roberta weights (/home/ubuntu/project/v2a-mapper/pretrain/clap_htsat_tiny.pt).\n"
     ]
    }
   ],
   "source": [
    "from audioldm.clap.encoders import CLAPAudioEmbeddingClassifierFreev2\n",
    "CLAP = CLAPAudioEmbeddingClassifierFreev2(\n",
    "    key='waveform',\n",
    "    pretrained_path=\"/home/ubuntu/project/v2a-mapper/pretrain/clap_htsat_tiny.pt\",\n",
    "    sampling_rate=16000,\n",
    "    embed_mode=\"audio\",\n",
    "    amodel=\"HTSAT-tiny\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = \"/mnt/new_volume/vgg_sound/test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change to column name\n",
    "# df_test = pd.read_csv(test_file, columns=[\"path\", \"label\"])\n",
    "df_test = pd.read_csv(test_file, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>glLQrEijrKg_000300.mp4</td>\n",
       "      <td>playing hammond organ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LDoXsip0BEQ_000177.mp4</td>\n",
       "      <td>parrot talking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6jiO0tPLK7U_000090.mp4</td>\n",
       "      <td>basketball bounce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cRlp5v9BHeE_000011.mp4</td>\n",
       "      <td>car passing by</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WgZ8KAnnTb8_000030.mp4</td>\n",
       "      <td>rapping</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15441</th>\n",
       "      <td>ZPeG32vKQ3w_000030.mp4</td>\n",
       "      <td>typing on typewriter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15442</th>\n",
       "      <td>yyqydavJ4YQ_000201.mp4</td>\n",
       "      <td>airplane flyby</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15443</th>\n",
       "      <td>EJwOhdjhxXY_000012.mp4</td>\n",
       "      <td>francolin calling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15444</th>\n",
       "      <td>oYEzy8gH6q8_000030.mp4</td>\n",
       "      <td>tapping guitar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15445</th>\n",
       "      <td>E75i9rHDHaE_000000.mp4</td>\n",
       "      <td>strike lighter</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15446 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            0                      1\n",
       "0      glLQrEijrKg_000300.mp4  playing hammond organ\n",
       "1      LDoXsip0BEQ_000177.mp4         parrot talking\n",
       "2      6jiO0tPLK7U_000090.mp4      basketball bounce\n",
       "3      cRlp5v9BHeE_000011.mp4         car passing by\n",
       "4      WgZ8KAnnTb8_000030.mp4                rapping\n",
       "...                       ...                    ...\n",
       "15441  ZPeG32vKQ3w_000030.mp4   typing on typewriter\n",
       "15442  yyqydavJ4YQ_000201.mp4         airplane flyby\n",
       "15443  EJwOhdjhxXY_000012.mp4      francolin calling\n",
       "15444  oYEzy8gH6q8_000030.mp4         tapping guitar\n",
       "15445  E75i9rHDHaE_000000.mp4         strike lighter\n",
       "\n",
       "[15446 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:199176 videos found in /mnt/new_volume/vgg_sound/scratch/shared/beegfs/hchen/train_data/VGGSound_final/video\n",
      "INFO:root:15446 videos found in /mnt/new_volume/vgg_sound/test.csv\n",
      "INFO:root:0 videos missing in /mnt/new_volume/vgg_sound/scratch/shared/beegfs/hchen/train_data/VGGSound_final/video\n"
     ]
    }
   ],
   "source": [
    "vgg_dataset = VGGSound(root=\"/mnt/new_volume/vgg_sound/scratch/shared/beegfs/hchen/train_data/VGGSound_final/video\",\n",
    "                           csv_path=\"/mnt/new_volume/vgg_sound/test.csv\",\n",
    "                           sample_rate =  16000)\n",
    "\n",
    "\n",
    "# data = vgg_dataset[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_rate: 44100\n",
      "self sample:  16000\n"
     ]
    }
   ],
   "source": [
    "a =vgg_dataset[0]['clip_video']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3, 224, 224])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v2a-mapper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
